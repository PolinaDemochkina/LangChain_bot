question,answer
Что такое LangChain и для чего он используется?,"LangChain - это фреймворк для разработки приложений, работающих на основе языковых моделей. Он позволяет создавать приложения, которые осведомлены о контексте и могут осуществлять рассуждения на основе предоставленного контекста."
Какие основные части включает в себя фреймворк LangChain?,"Фреймворк LangChain включает в себя библиотеки LangChain для Python и JavaScript, коллекцию LangChain Templates для развертывания архитектур, LangServe для развертывания цепочек LangChain в виде REST API, и LangSmith - платформу для отладки и тестирования."
Какие ключевые преимущества предлагают пакеты LangChain?,"Основные преимущества пакетов LangChain включают в себя компоненты для работы с языковыми моделями и готовые цепочки, которые облегчают начало работы и позволяют настраивать существующие цепочки или создавать новые."
Что такое LCEL в контексте LangChain?,"LCEL (LangChain Expression Language) - это декларативный способ составления цепочек в LangChain. Он был разработан для поддержки прототипов в производстве без изменений кода, от простых цепочек до более сложных."
Какие модули предоставляет LangChain?,"LangChain предоставляет стандартные, расширяемые интерфейсы и интеграции для модулей, таких как Model I/O для взаимодействия с языковыми моделями, Retrieval для работы с конкретными данными приложения и Agents для выбора инструментов моделями."
Какие примеры использования LangChain приведены в документации?,"В документации LangChain приведены примеры использования, такие как системы для ответов на вопросы по документам, чат-боты и анализ структурированных данных."
Какова роль LangChain в экосистеме инструментов?,"LangChain является частью богатой экосистемы инструментов, которые интегрируются с этим фреймворком и строятся на его основе, включая различные интеграции и руководства по лучшим практикам разработки."
Как установить LangChain с использованием Pip и Conda?,Для установки LangChain можно использовать Pip с командой 'pip install langchain' или Conda с командой 'conda install langchain -c conda-forge'. Это установит минимально необходимые требования LangChain.
Как установить LangChain из исходного кода?,"Для установки LangChain из исходного кода необходимо клонировать репозиторий и убедиться, что директория соответствует 'PATH/TO/REPO/langchain/libs/langchain', затем выполнить команду 'pip install -e .'."
Что представляет собой пакет 'langchain-experimental' и как его установить?,"Пакет 'langchain-experimental' содержит экспериментальный код LangChain, предназначенный для исследовательских и экспериментальных целей. Его можно установить с помощью команды 'pip install langchain-experimental'."
Как установить LangServe и для чего он используется?,"LangServe помогает разработчикам развертывать исполняемые файлы и цепочки LangChain в виде REST API. Он автоматически устанавливается с помощью LangChain CLI, или же его можно установить отдельно с помощью команды 'pip install ""langserve[all]""' для клиентских и серверных зависимостей."
Как установить LangChain CLI и в чем его предназначение?,LangChain CLI полезен для работы с шаблонами LangChain и другими проектами LangServe. Установить его можно с помощью команды 'pip install langchain-cli'.
Что такое LangSmith SDK и как его установить?,"LangSmith SDK автоматически устанавливается с LangChain. Если LangChain не используется, его можно установить отдельно с помощью команды 'pip install langsmith'."
Какие интеграции предлагает LangChain и как они способствуют созданию приложений?,"LangChain предлагает обширную экосистему интеграций с различными внешними ресурсами, такими как локальные и удаленные файловые системы, API и базы данных. Эти интеграции позволяют разработчикам создавать гибкие приложения, сочетающие возможности языковых моделей (LLM) с доступом, взаимодействием и манипулированием внешними ресурсами."
Какие лучшие практики безопасности следует учитывать при работе с LangChain?,"При работе с LangChain необходимо следовать лучшим практикам безопасности: ограничивать разрешения в соответствии с потребностями приложения, предвидеть потенциальное неправильное использование, применять многоуровневую защиту, используя, например, только чтение и методы изоляции, такие как использование контейнеров, для обеспечения безопасности доступа LLM к данным."
Какие риски существуют при недостаточной защите приложений LangChain и как их предотвратить?,"Риски при недостаточной защите включают потерю или повреждение данных, несанкционированный доступ к конфиденциальной информации, ухудшение производительности или доступности критических ресурсов. Чтобы предотвратить эти риски, можно ограничить доступ агентов к определенным директориям, использовать только для чтения API-ключи и ограничить доступ к базам данных только на чтение."
Как сообщить о проблеме безопасности в LangChain?,Проблемы безопасности в LangChain можно сообщать по электронной почте на адрес security@langchain.dev. Это обеспечит своевременное рассмотрение и необходимые действия.
Какие решения предлагает LangChain для корпоративных клиентов с особыми требованиями безопасности?,LangChain может предложить корпоративные решения для клиентов с дополнительными требованиями безопасности. Для обсуждения таких решений следует обратиться по адресу sales@langchain.dev.
Каковы основные методы и их назначение в протоколе 'Runnable' LangChain?,"Протокол 'Runnable' в LangChain включает основные методы: 'stream' для потоковой передачи частей ответа, 'invoke' для вызова цепочки на входных данных и 'batch' для вызова цепочки на списке входных данных. Существуют также асинхронные версии этих методов, такие как 'astream', 'ainvoke' и 'abatch'. Эти методы позволяют легко определять и вызывать пользовательские цепочки."
Как работает метод 'stream' в LangChain и каков его пример использования?,"Метод 'stream' в LangChain используется для потоковой передачи ответов модели в реальном времени. Например, использование 'chain.stream({""topic"": ""bears""})' позволит получать части ответа по мере их появления, что может быть полезно для отображения прогресса пользователю или для работы с промежуточными результатами."
Как используется метод 'invoke' в LangChain и приведите пример кода?,"Метод 'invoke' в LangChain используется для вызова цепочки на одном входном значении. Например, 'chain.invoke({""topic"": ""bears""})' вызовет цепочку для обработки темы 'bears', возвращая результат в виде строки или структурированного ответа."
Что делает метод 'batch' в LangChain и как его использовать?,"Метод 'batch' в LangChain позволяет обрабатывать несколько запросов одновременно. Например, использование 'chain.batch([{""topic"": ""bears""}, {""topic"": ""cats""}])' позволяет обрабатывать запросы на темы 'bears' и 'cats' параллельно, возвращая массив результатов."
Как настроить сервер и клиента для работы с LangServe?,"Для развертывания приложения LangChain как REST API можно использовать LangServe. Например, для создания сервера нужно определить цепочку, FastAPI приложение и маршрут:
```python
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseOutputParser
from langserve import add_routes

# Определение цепочки, приложения и маршрута
# ... код для CommaSeparatedListOutputParser и chat_prompt ...

app = FastAPI()
add_routes(app, category_chain, path=""/category_chain"")

if __name__ == ""__main__"":
    import uvicorn
    uvicorn.run(app, host=""localhost"", port=8000)
```
Также можно настроить клиента для программного взаимодействия с сервисом с помощью `langserve.RemoteRunnable`.
```python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable(""http://localhost:8000/category_chain/"")
remote_chain.invoke({""text"": ""colors""})
```"
Как импортировать и использовать LLM и ChatModel в LangChain?,Для работы с разными типами моделей в LangChain нужно импортировать `OpenAI` и `ChatOpenAI` из `langchain.llms` и `langchain.chat_models` соответственно. Пример кода: from langchain.llms import OpenAI; from langchain.chat_models import ChatOpenAI; llm = OpenAI(); chat_model = ChatOpenAI().
Как создается и используется шаблон запроса (Prompt Template) в LangChain?,"В LangChain шаблоны запросов используются для формирования полноценных запросов к языковым моделям. Пример кода: from langchain.prompts import PromptTemplate; prompt = PromptTemplate.from_template(""What is a good name for a company that makes {product}?""); prompt.format(product=""colorful socks"")."
Что делают парсеры вывода (Output Parsers) в LangChain и как их создать?,"Парсеры вывода в LangChain преобразуют сырой ответ языковой модели в формат, который можно использовать далее. Пример кода для создания парсера: from langchain.schema import BaseOutputParser; class CommaSeparatedListOutputParser(BaseOutputParser): def parse(self, text: str): return text.strip().split("", "")."
Как сочетаются компоненты в LangChain с использованием LCEL?,"В LangChain можно комбинировать компоненты, используя LangChain Expression Language (LCEL). Пример кода: from langchain.chat_models import ChatOpenAI; from langchain.prompts import ChatPromptTemplate; from langchain.schema import BaseOutputParser; chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser(); chain.invoke({""text"": ""colors""})."
Что такое протокол 'Runnable' в LangChain и где он реализован?,"Протокол 'Runnable' в LangChain - это стандартный интерфейс, реализованный для большинства компонентов. Он облегчает создание пользовательских цепочек и их вызов стандартным способом."
Какие методы включает стандартный интерфейс 'Runnable' в LangChain?,"Стандартный интерфейс 'Runnable' в LangChain включает методы: 'stream' для возврата частей ответа, 'invoke' для вызова цепочки на входных данных и 'batch' для вызова цепочки на списке входных данных."
Какие асинхронные методы поддерживает протокол 'Runnable' в LangChain?,"Протокол 'Runnable' в LangChain поддерживает асинхронные методы: 'astream' для асинхронного возврата частей ответа, 'ainvoke' для асинхронного вызова цепочки на входных данных и 'abatch' для асинхронного вызова цепочки на списке входных данных."
Какие типы входных и выходных данных поддерживаются различными компонентами протокола 'Runnable' в LangChain?,"Типы входных и выходных данных варьируются в зависимости от компонента в LangChain. Например, компонент 'Prompt' принимает словарь и возвращает 'PromptValue', а 'ChatModel' принимает одну строку или список сообщений чата и возвращает 'ChatMessage'."
Что такое схемы ввода и вывода в контексте 'Runnable' в LangChain и как они генерируются?,"Схемы ввода и вывода в 'Runnable' в LangChain - это Pydantic модели, автоматически генерируемые из структуры 'Runnable'. Они предоставляют представление JSONSchema для инспекции входных и выходных данных компонентов."
Какие шаги необходимо выполнить для создания и использования простой цепочки 'PromptTemplate + ChatModel' в LangChain?,"Для создания и использования цепочки 'PromptTemplate + ChatModel' в LangChain, необходимо импортировать 'ChatOpenAI' и 'ChatPromptTemplate', создать экземпляр модели и шаблона приглашения, а затем объединить их в цепочку."
Как создать и использовать цепочку 'PromptTemplate + ChatModel' в LangChain?,"Чтобы создать и использовать цепочку 'PromptTemplate + ChatModel' в LangChain, выполните следующие шаги:
1. Импортируйте необходимые модули:
```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
```
2. Создайте экземпляры модели и шаблона приглашения:
```python
model = ChatOpenAI()
prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")
```
3. Объедините их в цепочку:
```python
chain = prompt | model
```"
Как использовать метод 'stream' для потоковой передачи частей ответа в цепочке LangChain?,"Чтобы использовать метод 'stream' для потоковой передачи частей ответа, используйте следующий код:
```python
for s in chain.stream({""topic"": ""bears""}):
    print(s.content, end="""", flush=True)
```
Этот код выводит части ответа по мере их получения из цепочки, обрабатывающей запрос на тему 'bears'."
Как работает метод 'invoke' для вызова цепочки на входных данных в LangChain?,"Метод 'invoke' вызывает цепочку на входных данных. Пример использования:
```python
chain.invoke({""topic"": ""bears""})
```
Этот код вызовет цепочку для обработки запроса на тему 'bears' и вернет результат."
Как использовать метод 'batch' для вызова цепочки на списке входных данных в LangChain?,"Чтобы использовать метод 'batch' для вызова цепочки на списке входных данных, используйте следующий код:
```python
chain.batch([{""topic"": ""bears""}, {""topic"": ""cats""}])
```
Этот код вызовет цепочку для каждого элемента в списке (в данном случае для тем 'bears' и 'cats') и вернет список результатов."
Как можно асинхронно использовать метод 'astream' в цепочке LangChain?,"Для асинхронного использования метода 'astream' в цепочке LangChain, примените следующий код:
```python
async for s in chain.astream({""topic"": ""bears""}):
    print(s.content, end="""", flush=True)
```
Этот асинхронный код будет выводить части ответа по мере их получения для запроса на тему 'bears'."
Что такое привязка аргументов во время выполнения в LangChain?,"Иногда нам нужно вызвать Runnable в последовательности Runnable с постоянными аргументами, которые не являются частью вывода предыдущего Runnable в последовательности и которые не являются частью пользовательского ввода. Мы можем использовать `Runnable.bind()`, чтобы легко передать эти аргументы."
Как создать простую последовательность подсказки и модели в LangChain?,"Допустим, у нас есть простая последовательность подсказки + модели:

            from langchain.chat_models import ChatOpenAI
            from langchain.prompts import ChatPromptTemplate
            from langchain.schema import StrOutputParser
            from langchain.schema.runnable import RunnablePassthrough

            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        ""system"",
                        ""Write out the following equation using algebraic symbols then solve it. Use the format

EQUATION:...
SOLUTION:...

"",
                    ),
                    (""human"", ""{equation_statement}""),
                ]
            )
            model = ChatOpenAI(temperature=0)
            runnable = (
                {""equation_statement"": RunnablePassthrough()} | prompt | model | StrOutputParser()
            )

            print(runnable.invoke(""x raised to the third plus seven equals 12""))"
Как вызвать модель с определенными 'стоп' словами в LangChain?,"и хотим вызвать модель с определенными 'стоп' словами:

            runnable = (
                {""equation_statement"": RunnablePassthrough()}
                | prompt
                | model.bind(stop=""SOLUTION"")
                | StrOutputParser()
            )
            print(runnable.invoke(""x raised to the third plus seven equals 12""))"
Каково полезное применение привязки в LangChain?,"Одно из полезных применений привязки - это прикрепление функций OpenAI к совместимой модели OpenAI:

            function = {
                ""name"": ""solver"",
                ""description"": ""Formulates and solves an equation"",
                ""parameters"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""equation"": {
                            ""type"": ""string"",
                            ""description"": ""The algebraic expression of the equation"",
                        },
                        ""solution"": {
                            ""type"": ""string"",
                            ""description"": ""The solution to the equation"",
                        },
                    },
                    ""required"": [""equation"", ""solution""],
                },
            }

            # Need gpt-4 to solve this one correctly
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        ""system"",
                        ""Write out the following equation using algebraic symbols then solve it."",
                    ),
                    (""human"", ""{equation_statement}""),
                ]
            )
            model = ChatOpenAI(model=""gpt-4"", temperature=0).bind(
                function_call={""name"": ""solver""}, functions=[function]
            )
            runnable = {""equation_statement"": RunnablePassthrough()} | prompt | model
            runnable.invoke(""x raised to the third plus seven equals 12"")"
Какие методы определены для настройки внутренних компонентов цепочки в LangChain?,"
            Часто вы можете захотеть экспериментировать или даже предоставить конечному пользователю несколько различных способов выполнения задач. Для упрощения этого процесса мы определили два метода. 
            Первый - метод `configurable_fields`, который позволяет настраивать определенные поля runnable. 
            Второй - метод `configurable_alternatives`, с помощью которого можно перечислить альтернативы для любого конкретного runnable, которые могут быть установлены во время выполнения.
        "
Как настроить поля конфигурации с LLM в LangChain?,"С LLM мы можем настраивать такие вещи, как температура:

            from langchain.chat_models import ChatOpenAI
            from langchain.prompts import PromptTemplate

            model = ChatOpenAI(temperature=0).configurable_fields(
                temperature=ConfigurableField(
                    id=""llm_temperature"",
                    name=""LLM Temperature"",
                    description=""The temperature of the LLM"",
                )
            )

            model.invoke(""pick a random number"")
                AIMessage(content='7')

            model.with_config(configurable={""llm_temperature"": 0.9}).invoke(""pick a random number"")
                AIMessage(content='34')"
Как использовать `configurable_alternatives` с LLM в LangChain?,"Давайте рассмотрим это с LLM:

            from langchain.chat_models import ChatAnthropic, ChatOpenAI
            from langchain.prompts import PromptTemplate
            from langchain.schema.runnable import ConfigurableField

            llm = ChatAnthropic(temperature=0).configurable_alternatives(
                ConfigurableField(id=""llm""),
                default_key=""anthropic"",
                openai=ChatOpenAI(),
                gpt4=ChatOpenAI(model=""gpt-4""),
            )
            prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"")
            chain = prompt | llm

            chain.invoke({""topic"": ""bears""})
                AIMessage(content=""Here's a silly joke about bears: ..."")

            chain.with_config(configurable={""llm"": ""openai""}).invoke({""topic"": ""bears""})
                AIMessage(content=""Sure, here's a bear joke for you: ..."")"
Как сохранить настроенные цепочки как собственные объекты в LangChain?,"
            Мы также можем легко сохранять настроенные цепочки как собственные объекты:

            openai_poem = chain.with_config(configurable={""llm"": ""openai""})

            openai_poem.invoke({""topic"": ""bears""})
                AIMessage(content=""Why don't bears wear shoes? ..."")
        "
Какова цель использования запасных вариантов в приложениях LLM в LangChain?,"
            В приложениях LLM существует множество возможных точек сбоя, будь то проблемы с API LLM, плохие результаты модели, проблемы с другими интеграциями и т.д. Запасные варианты помогают элегантно обрабатывать и изолировать эти проблемы.
            Важно, что запасные варианты могут применяться не только на уровне LLM, но и на уровне всего runnable.
        "
Как обрабатывать ошибки API LLM в LangChain?,"Это, возможно, самый распространенный случай использования запасных вариантов. Запрос к API LLM может завершиться неудачей по разным причинам - API может быть недоступен, вы можете достичь ограничения по скорости, и так далее.

            from langchain.chat_models import ChatOpenAI
            from langchain.prompts import PromptTemplate
            from langchain.schema.runnable import Fallback, RunnableException

            prompt = PromptTemplate.from_template(""What is the capital of {country}?"")
            model = ChatOpenAI(temperature=0)

            chain = prompt | Fallback(model, lambda _: ""Sorry, I'm having trouble accessing my resources right now."")

            chain.invoke({""country"": ""France""})
                AIMessage(content='The capital of France is Paris.')

            model.with_exception(RunnableException()).invoke({""country"": ""France""})
                AIMessage(content='Sorry, I'm having trouble accessing my resources right now.')"
Как указать ошибки для обработки при использовании запасных вариантов в LangChain?,"
            Мы также можем указать ошибки для обработки, если хотим быть более конкретными в отношении того, когда должен вызываться запасной вариант:

            from langchain.schema.runnable import FallbackError

            chain = prompt | Fallback(model, lambda _: ""Sorry, I can't help with that right now."", error_types=[FallbackError])
        "
Как создать запасные варианты для последовательностей в LangChain?,"Мы также можем создавать запасные варианты для последовательностей, которые сами по себе являются последовательностями. 

            from langchain.schema.runnable import RunnableSequence

            chain2 = prompt | model2
            chain_fallback = RunnableSequence([chain, chain2], use_fallback=True)

            chain_fallback.invoke({""country"": ""France""})
                AIMessage(content='The capital of France is Paris.')"
Как использовать произвольные функции в конвейере LangChain?,"```python
from operator import itemgetter   

from langchain.chat_models import ChatOpenAI   
from langchain.prompts import ChatPromptTemplate   
from langchain.schema.runnable import RunnableLambda   

def length_function(text):  
    return len(text)  

def _multiple_length_function(text1, text2):  
    return len(text1) * len(text2)  

def multiple_length_function(_dict):  
    return _multiple_length_function(_dict[""text1""], _dict[""text2""])  

prompt = ChatPromptTemplate.from_template(""what is {a} + {b}"")  
model = ChatOpenAI()  

chain1 = prompt | model   

chain = (  
    {  
        ""a"": itemgetter(""foo"") | RunnableLambda(length_function),  
        ""b"": {""text1"": itemgetter(""foo""), ""text2"": itemgetter(""bar"")}  
        | RunnableLambda(multiple_length_function),  
    }  
    | prompt  
    | model  
)  

chain.invoke({""foo"": ""bar"", ""bar"": ""gah""})  

    AIMessage(content='3 + 9 equals 12.', additional_kwargs={}, example=False)
```"
"Как использовать RunnableConfig с RunnableLambda для передачи обратных вызовов, тегов и другой конфигурационной информации в LangChain?","```python
from langchain.schema.output_parser import StrOutputParser   
from langchain.schema.runnable import RunnableConfig   

import json   

def parse_or_fix(text: str, config: RunnableConfig):  
    fixing_chain = (  
        ChatPromptTemplate.from_template(  
            ""Fix the following text:

```text
{input}
```
Error: {error}""  
            "" Don't narrate, just respond with the fixed data.""  
        )  
        | ChatOpenAI()  
        | StrOutputParser()  
    )  
    for _ in range(3):  
        try:  
            return json.loads(text)  
        except Exception as e:  
            text = fixing_chain.invoke({""input"": text, ""error"": e}, config)  
    return ""Failed to parse""  

from langchain.callbacks import get_openai_callback   

with get_openai_callback() as cb:  
    RunnableLambda(parse_or_fix).invoke(  
        ""{foo: bar}"", {""tags"": [""my-tag""], ""callbacks"": [cb]}  
    )  
    print(cb)  

    Tokens Used: 65  
        Prompt Tokens: 56  
        Completion Tokens: 9  
    Successful Requests: 1  
    Total Cost (USD): $0.00010200000000000001
```"
Какие функции можно использовать в LCEL-пайплайне?,"Вы можете использовать функции-генераторы (т.е. функции, которые используют ключевое слово `yield` и ведут себя как итераторы) в LCEL-пайплайне."
Какова сигнатура этих генераторов?,Сигнатура этих генераторов должна быть `Iterator[Input] -> Iterator[Output]`. Или для асинхронных генераторов: `AsyncIterator[Input] -> AsyncIterator[Output]`.
"Как реализовать настраиваемый парсер вывода для списков, разделенных запятыми?","from typing import Iterator, List   

from langchain.chat_models import ChatOpenAI   
from langchain.prompts.chat import ChatPromptTemplate   
from langchain.schema.output_parser import StrOutputParser   

prompt = ChatPromptTemplate.from_template(  
    ""Write a comma-separated list of 5 animals similar to: {animal}""  
)  
model = ChatOpenAI(temperature=0.0)  

str_chain = prompt | model | StrOutputParser()  

for chunk in str_chain.stream({""animal"": ""bear""}):  
    print(chunk, end="""", flush=True)  

    lion, tiger, wolf, gorilla, panda   

str_chain.invoke({""animal"": ""bear""})  

    'lion, tiger, wolf, gorilla, panda'"
"Как работает настраиваемый парсер, который разделяет итератор токенов llm на список строк, разделенных запятыми?","# This is a custom parser that splits an iterator of llm tokens  
# into a list of strings separated by commas   
def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:  
    # hold partial input until we get a comma  
    buffer = """"  
    for chunk in input:  
        # add current chunk to buffer  
        buffer += chunk  
        # while there are commas in the buffer  
        while "","" in buffer:  
            # split buffer on comma  
            comma_index = buffer.index("","")  
            # yield everything before the comma  
            yield [buffer[:comma_index].strip()]  
            # save the rest for the next iteration  
            buffer = buffer[comma_index + 1 :]  
    # yield the last chunk  
    yield [buffer.strip()]  

list_chain = str_chain | split_into_list   

for chunk in list_chain.stream({""animal"": ""bear""}):  
    print(chunk, flush=True)  

    ['lion']  
    ['tiger']  
    ['wolf']  
    ['gorilla']  
    ['panda']  

list_chain.invoke({""animal"": ""bear""})  

    ['lion', 'tiger', 'wolf', 'gorilla', 'panda']"
Как работает RunnableParallel (или RunnableMap) для выполнения нескольких Runnable параллельно?,"RunnableParallel (или RunnableMap) позволяет легко выполнять несколько Runnable параллельно и возвращать результаты этих Runnable в виде карты.

from langchain.chat_models import ChatOpenAI   
from langchain.prompts import ChatPromptTemplate   
from langchain.schema.runnable import RunnableParallel   

model = ChatOpenAI()  
joke_chain = ChatPromptTemplate.from_template(""tell me a joke about {topic}"") | model   
poem_chain = (  
    ChatPromptTemplate.from_template(""write a 2-line poem about {topic}"") | model  
)  

map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)  

map_chain.invoke({""topic"": ""bear""})  

    {'joke': AIMessage(content=""Why don't bears wear shoes? 

Because they have bear feet!"", additional_kwargs={}, example=False),  
     'poem': AIMessage(content=""In woodland depths, bear prowls with might,
Silent strength, nature's sovereign, day and night."", additional_kwargs={}, example=False)}"
Как карты могут быть полезны для манипулирования выводом одного Runnable для соответствия формату ввода следующего Runnable в последовательности?,"Карты могут быть полезны для манипулирования выводом одного Runnable, чтобы соответствовать формату ввода следующего Runnable в последовательности.

from langchain.embeddings import OpenAIEmbeddings   
from langchain.schema.output_parser import StrOutputParser   
from langchain.schema.runnable import RunnablePassthrough   
from langchain.vectorstores import FAISS   

vectorstore = FAISS.from_texts(  
    [""harrison worked at kensho""], embedding=OpenAIEmbeddings()  
)  
retriever = vectorstore.as_retriever()  
template = ""Answer the question based only on the following context:  
{context}  

Question: {question}  
""  
prompt = ChatPromptTemplate.from_template(template)  

retrieval_chain = (  
    {""context"": retriever, ""question"": RunnablePassthrough()}  
    | prompt  
    | model  
    | StrOutputParser()  
)  

retrieval_chain.invoke(""where did harrison work?"")  

    'Harrison worked at Kensho.'"
Как RunnableMaps могут быть полезны для параллельного выполнения независимых процессов?,"RunnableMaps полезны для параллельного выполнения независимых процессов, поскольку каждый Runnable в карте выполняется параллельно. Например, joke_chain, poem_chain и map_chain имеют примерно одинаковое время выполнения, даже если map_chain выполняет оба других.

joke_chain.invoke({""topic"": ""bear""})  

    958 ms ± 402 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  

poem_chain.invoke({""topic"": ""bear""})  

    1.22 s ± 508 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  

map_chain.invoke({""topic"": ""bear""})  

    1.15 s ± 119 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
Как инициализируется и работает `RunnableBranch`?,"`RunnableBranch` инициализируется списком пар (условие, runnable) и резервным runnable. Он выбирает ветку, передавая каждому условию ввод, с которым он вызывается. Он выбирает первое условие, которое оценивается как True, и запускает соответствующий runnable для этого условия с вводом. Если ни одно из предоставленных условий не подходит, он запускает резервный runnable.

from langchain.chat_models import ChatAnthropic   
from langchain.prompts import PromptTemplate   
from langchain.schema.output_parser import StrOutputParser   

chain = (  
    PromptTemplate.from_template(  
        """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.  

Do not respond with more than one word.  

<question>  
{question}  
</question>  

Classification:""""""  
    )  
    | ChatAnthropic()  
    | StrOutputParser()  
)  

chain.invoke({""question"": ""how do I call Anthropic?""})  

    ' Anthropic'"
Как создать подцепочки и динамически направлять логику на основе входных данных?,"Можно создать три подцепочки и динамически направлять логику на основе входных данных.

langchain_chain = (  
    PromptTemplate.from_template(  
        """"""You are an expert in langchain. \  
Always answer questions starting with ""As Harrison Chase told me"". \  
Respond to the following question:  

Question: {question}  
Answer:""""""  
    )  
    | ChatAnthropic()  
)  
anthropic_chain = (  
    PromptTemplate.from_template(  
        """"""You are an expert in anthropic. \  
Always answer questions starting with ""As Dario Amodei told me"". \  
Respond to the following question:  

Question: {question}  
Answer:""""""  
    )  
    | ChatAnthropic()  
)  
general_chain = (  
    PromptTemplate.from_template(  
        """"""Respond to the following question:  

Question: {question}  
Answer:""""""  
    )  
    | ChatAnthropic()  
)  

from langchain.schema.runnable import RunnableBranch   

branch = RunnableBranch(  
    (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain),  
    (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain),  
    general_chain,  
)  

full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch   

full_chain.invoke({""question"": ""how do I use Anthropic?""})  

    AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic: ..."")"
Как использовать пользовательскую функцию для динамического маршрутизирования логики?,"Можно использовать пользовательскую функцию для динамического маршрутизирования логики между различными выходами.

def route(info):  
    if ""anthropic"" in info[""topic""].lower():  
        return anthropic_chain  
    elif ""langchain"" in info[""topic""].lower():  
        return langchain_chain  
    else:  
        return general_chain   

from langchain.schema.runnable import RunnableLambda   

full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda(  
    route  
)  

full_chain.invoke({""question"": ""how do I use Anthroipc?""})  

    AIMessage(content=' As Dario Amodei told me, to use Anthropic IPC you first need to import it: ...')"
Какова самая простая композиция с использованием PromptTemplate и LLM?,"Самая простая композиция включает сочетание шаблона запроса и модели для создания цепочки, которая принимает пользовательский ввод, добавляет его к запросу, передает его модели и возвращает сырой вывод модели.

from langchain.chat_models import ChatOpenAI   
from langchain.prompts import ChatPromptTemplate   

prompt = ChatPromptTemplate.from_template(""tell me a joke about {foo}"")  
model = ChatOpenAI()  
chain = prompt | model   

chain.invoke({""foo"": ""bears""})  

    AIMessage(content=""Why don't bears wear shoes?

Because they have bear feet!"", additional_kwargs={}, example=False)"
Как прикрепить последовательности остановки к цепочке?,"Можно прикрепить последовательности остановки к цепочке.

chain = prompt | model.bind(stop=[""
""])  

chain.invoke({""foo"": ""bears""})  

    AIMessage(content='Why did the bear never wear shoes?', additional_kwargs={}, example=False)"
Как прикрепить информацию о вызове функции к цепочке?,"Можно прикрепить информацию о вызове функции к цепочке.

functions = [  
    {  
        ""name"": ""joke"",  
        ""description"": ""A joke"",  
        ""parameters"": {  
            ""type"": ""object"",  
            ""properties"": {  
                ""setup"": {""type"": ""string"", ""description"": ""The setup for the joke""},  
                ""punchline"": {  
                    ""type"": ""string"",  
                    ""description"": ""The punchline for the joke"",  
                },  
            },  
            ""required"": [""setup"", ""punchline""],  
        },  
    }  
]  
chain = prompt | model.bind(function_call={""name"": ""joke""}, functions=functions)  

chain.invoke({""foo"": ""bears""}, config={})  

    AIMessage(content='', additional_kwargs={'function_call': {'name': 'joke', 'arguments': '{
  ""setup"": ""Why don't bears wear shoes?"",
  ""punchline"": ""Because they have bear feet!""
}'}}, example=False)"
"Как добавить этап извлечения информации к запросу и LLM, создавая цепочку с расширенной генерацией на основе извлечения?","Можно добавить этап извлечения информации к запросу и LLM, создавая цепочку с расширенной генерацией на основе извлечения.

pip install langchain openai faiss-cpu tiktoken   

from operator import itemgetter   

from langchain.chat_models import ChatOpenAI   
from langchain.embeddings import OpenAIEmbeddings   
from langchain.prompts import ChatPromptTemplate   
from langchain.schema.output_parser import StrOutputParser   
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough   
from langchain.vectorstores import FAISS   

vectorstore = FAISS.from_texts(  
    [""harrison worked at kensho""], embedding=OpenAIEmbeddings()  
)  
retriever = vectorstore.as_retriever()  

template = """"""Answer the question based only on the following context:  
{context}  

Question: {question}  
""""""  
prompt = ChatPromptTemplate.from_template(template)  

model = ChatOpenAI()  

chain = (  
    {""context"": retriever, ""question"": RunnablePassthrough()}  
    | prompt  
    | model  
    | StrOutputParser()  
)  

chain.invoke(""where did harrison work?"")  

    'Harrison worked at Kensho.'【45†source】"
Как создать цепочку извлечения информации для разговоров?,"Можно легко добавить историю беседы. Это в основном означает добавление chat_message_history.

from langchain.schema import format_document   
from langchain.schema.runnable import RunnableMap   

from langchain.prompts.prompt import PromptTemplate   

_template = """"""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.  

Chat History:  
{chat_history}  
Follow Up Input: {question}  
Standalone question:""""""  
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)  

template = """"""Answer the question based only on the following context:  
{context}  

Question: {question}  
""""""  
ANSWER_PROMPT = ChatPromptTemplate.from_template(template)  

DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=""{page_content}"")  

def _combine_documents(  
    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=""

""  
):  
    doc_strings = [format_document(doc, document_prompt) for doc in docs]  
    return document_separator.join(doc_strings)  

from typing import List, Tuple   

def _format_chat_history(chat_history: List[Tuple]) -> str:  
    buffer = """"  
    for dialogue_turn in chat_history:  
        human = ""Human: "" + dialogue_turn[0]  
        ai = ""Assistant: "" + dialogue_turn[1]  
        buffer += ""
"" + ""
"".join([human, ai])  
    return buffer   

_inputs = RunnableMap(  
    standalone_question=RunnablePassthrough.assign(  
        chat_history=lambda x: _format_chat_history(x[""chat_history""])  
    )  
    | CONDENSE_QUESTION_PROMPT  
    | ChatOpenAI(temperature=0)  
    | StrOutputParser(),  
)  
_context = {  
    ""context"": itemgetter(""standalone_question"") | retriever | _combine_documents,  
    ""question"": lambda x: x[""standalone_question""],  
}  
conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()  

conversational_qa_chain.invoke(  
    {  
        ""question"": ""where did harrison work?"",  
        ""chat_history"": [],  
    }  
)  

    AIMessage(content='Harrison was employed at Kensho.', additional_kwargs={}, example=False)  

conversational_qa_chain.invoke(  
    {  
        ""question"": ""where did he work?"",  
        ""chat_history"": [(""Who wrote this notebook?"", ""Harrison"")],  
    }  
)  

    AIMessage(content='Harrison worked at Kensho.', additional_kwargs={}, example=False)【46†source】"
Как использовать память и возвращать исходные документы в цепочке извлечения?,"Это показывает, как использовать память с вышеуказанным. Для памяти нам нужно управлять ею снаружи, а для возврата извлеченных документов просто нужно пропустить их через всю цепочку.

from operator import itemgetter   

from langchain.memory import ConversationBufferMemory   

memory = ConversationBufferMemory(  
    return_messages=True, output_key=""answer"", input_key=""question""  
)  

# First we add a step to load memory  
# This adds a ""memory"" key to the input object   
loaded_memory = RunnablePassthrough.assign(  
    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(""history""),  
)  
# Now we calculate the standalone question   
standalone_question = {  
    ""standalone_question"": {  
        ""question"": lambda x: x[""question""],  
        ""chat_history"": lambda x: _format_chat_history(x[""chat_history""]),  
    }  
    | CONDENSE_QUESTION_PROMPT  
    | ChatOpenAI(temperature=0)  
    | StrOutputParser(),  
}  
# Now we retrieve the documents   
retrieved_documents = {  
    ""docs"": itemgetter(""standalone_question"") | retriever,  
    ""question"": lambda x: x[""standalone_question""],  
}  
# Now we construct the inputs for the final prompt   
final_inputs = {  
    ""context"": lambda x: _combine_documents(x[""docs""]),  
    ""question"": itemgetter(""question""),  
}  
# And finally, we do the part that returns the answers   
answer = {  
    ""answer"": final_inputs | ANSWER_PROMPT | ChatOpenAI(),  
    ""docs"": itemgetter(""docs""),  
}  
# And now we put it all together!  
final_chain = loaded_memory | standalone_question | retrieved_documents | answer   

inputs = {""question"": ""where did harrison work?""}  
result = final_chain.invoke(inputs)  
result  

    {'answer': AIMessage(content='Harrison was employed at Kensho.', additional_kwargs={}, example=False),  
     'docs': [Document(page_content='harrison worked at kensho', metadata={})]}  

# Note that the memory does not save automatically  
# This will be improved in the future  
# For now you need to save it yourself   
memory.save_context(inputs, {""answer"": result[""answer""].content})  

memory.load_memory_variables({})  

    {'history': [HumanMessage(content='where did harrison work?', additional_kwargs={}, example=False),  ...]}"
Как использовать Runnable для объединения нескольких цепочек?,"Runnable можно легко использовать для объединения нескольких цепочек.

        from operator import itemgetter   

        from langchain.chat_models import ChatOpenAI   
        from langchain.prompts import ChatPromptTemplate   
        from langchain.schema import StrOutputParser   

        prompt1 = ChatPromptTemplate.from_template(""what is the city {person} is from?"")  
        prompt2 = ChatPromptTemplate.from_template(  
            ""what country is the city {city} in? respond in {language}""  
        )  

        model = ChatOpenAI()  

        chain1 = prompt1 | model | StrOutputParser()  

        chain2 = (  
            {""city"": chain1, ""language"": itemgetter(""language"")}  
            | prompt2  
            | model  
            | StrOutputParser()  
        )  

        chain2.invoke({""person"": ""obama"", ""language"": ""spanish""})  

            'El país donde se encuentra la ciudad de Honolulu, donde nació Barack Obama, el 44º Presidente de los Estados Unidos, es Estados Unidos. Honolulu se encuentra en la isla de Oahu, en el estado de Hawái.'"
Как создать ветвление и слияние в цепочках?,"Вы можете захотеть, чтобы вывод одного компонента обрабатывался двумя или более другими компонентами. RunnableMaps позволяют разделить или разветвить цепочку, чтобы несколько компонентов могли параллельно обрабатывать ввод. Позже другие компоненты могут объединиться или смешать результаты для синтеза окончательного ответа. Такой тип цепочки создает граф вычислений, который выглядит следующим образом:

             Input  
              / \  
             /   \  
         Branch1 Branch2  
             \   /  
              \ /  
              Combine   

        planner = (  
            ChatPromptTemplate.from_template(""Generate an argument about: {input}"")  
            | ChatOpenAI()  
            | StrOutputParser()  
            | {""base_response"": RunnablePassthrough()}  
        )  

        arguments_for = (  
            ChatPromptTemplate.from_template(  
                ""List the pros or positive aspects of {base_response}""  
            )  
            | ChatOpenAI()  
            | StrOutputParser()  
        )  
        arguments_against = (  
            ChatPromptTemplate.from_template(  
                ""List the cons or negative aspects of {base_response}""  
            )  
            | ChatOpenAI()  
            | StrOutputParser()  
        )  

        final_responder = (  
            ChatPromptTemplate.from_messages(  
                [  
                    (""ai"", ""{original_response}""),  
                    (""human"", ""Pros:
{results_1}

Cons:
{results_2}""),  
                    (""system"", ""Generate a final response given the critique""),  
                ]  
            )  
            | ChatOpenAI()  
            | StrOutputParser()  
        )  

        chain = (  
            planner  
            | {  
                ""results_1"": arguments_for,  
                ""results_2"": arguments_against,  
                ""original_response"": itemgetter(""base_response""),  
            }  
            | final_responder  
        )  

        chain.invoke({""input"": ""scrum""})  

            'While Scrum has its potential cons and challenges, many organizations have successfully embraced and implemented this project management framework to great effect. ...'"
Как использовать Runnables для воспроизведения SQLDatabaseChain?,"Для воспроизведения SQLDatabaseChain можно использовать Runnables.

from langchain.prompts import ChatPromptTemplate   

template = """"""Based on the table schema below, write a SQL query that would answer the user's question:  
{schema}  

Question: {question}  
SQL Query:""""""  
prompt = ChatPromptTemplate.from_template(template)  

from langchain.utilities import SQLDatabase   

db = SQLDatabase.from_uri(""sqlite:///./Chinook.db"")  

def get_schema(_):  
    return db.get_table_info()  

def run_query(query):  
    return db.run(query)  

from langchain.chat_models import ChatOpenAI   
from langchain.schema.output_parser import StrOutputParser   
from langchain.schema.runnable import RunnablePassthrough   

model = ChatOpenAI()  

sql_response = (  
    RunnablePassthrough.assign(schema=get_schema)  
    | prompt  
    | model.bind(stop=[""
SQLResult:""])  
    | StrOutputParser()  
)  

sql_response.invoke({""question"": ""How many employees are there?""})  

    'SELECT COUNT(*) FROM Employee'  

template = """"""Based on the table schema below, question, sql query, and sql response, write a natural language response:  
{schema}  

Question: {question}  
SQL Query: {query}  
SQL Response: {response}""""""  
prompt_response = ChatPromptTemplate.from_template(template)  

full_chain = (  
    RunnablePassthrough.assign(query=sql_response)  
    | RunnablePassthrough.assign(  
        schema=get_schema,  
        response=lambda x: db.run(x[""query""]),  
    )  
    | prompt_response  
    | model  
)  

full_chain.invoke({""question"": ""How many employees are there?""})  

    AIMessage(content='There are 8 employees.', additional_kwargs={}, example=False)"
Как передать Runnable в агента?,"Можно передать Runnable в агента.

from langchain.agents import AgentExecutor, XMLAgent, tool   
from langchain.chat_models import ChatAnthropic   

model = ChatAnthropic(model=""claude-2"")  

@tool   
def search(query: str) -> str:  
    """"""Search things about current events.""""""  
    return ""32 degrees""  

tool_list = [search]  

# Get prompt to use   
prompt = XMLAgent.get_default_prompt()  

# Logic for going from intermediate steps to a string to pass into model  
# This is pretty tied to the prompt   
def convert_intermediate_steps(intermediate_steps):  
    log = """"  
    for action, observation in intermediate_steps:  
        log += (  
            f""<tool>{action.tool}</tool><tool_input>{action.tool_input}""  
            f""</tool_input><observation>{observation}</observation>""  
        )  
    return log  

# Logic for converting tools to string to go in prompt   
def convert_tools(tools):  
    return ""
"".join([f""{tool.name}: {tool.description}"" for tool in tools])  

agent = (  
    {  
        ""question"": lambda x: x[""question""],  
        ""intermediate_steps"": lambda x: convert_intermediate_steps(  
            x[""intermediate_steps""]  
        ),  
    }  
    | prompt.partial(tools=convert_tools(tool_list))  
    | model.bind(stop=[""</tool_input>"", ""</final_answer>""])  
    | XMLAgent.get_default_output_parser()  
)  

agent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)  

agent_executor.invoke({""question"": ""whats the weather in New york?""})  

    > Entering new AgentExecutor chain...  
     <tool>search</tool>  
    <tool_input>weather in new york32 degrees  

    <final_answer>The weather in New York is 32 degrees  

    > Finished chain.  

    {'question': 'whats the weather in New york?',  
     'output': 'The weather in New York is 32 degrees'}"
Как использовать LCEL для написания кода на Python?,"Пример использования LCEL для написания кода на Python.

from langchain.chat_models import ChatOpenAI   
from langchain.prompts import ChatPromptTemplate  
from langchain.schema.output_parser import StrOutputParser   
from langchain_experimental.utilities import PythonREPL   

template = """"""Write some python code to solve the user's problem.   

Return only python code in Markdown format, e.g.:  

```python  
....  
```""""""  
prompt = ChatPromptTemplate.from_messages([(""system"", template), (""human"", ""{input}"")])  

model = ChatOpenAI()  

def _sanitize_output(text: str):  
    _, after = text.split(""```python"")  
    return after.split(""```"")[0]  

chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run   

chain.invoke({""input"": ""whats 2 plus 2""})  

    Python REPL can execute arbitrary code. Use with caution.  

    '4
'"
Как использовать LCEL для маршрутизации по семантической близости?,"Пример использования LCEL для маршрутизации по семантической близости.

from langchain.chat_models import ChatOpenAI   
from langchain.embeddings import OpenAIEmbeddings   
from langchain.prompts import PromptTemplate   
from langchain.schema.output_parser import StrOutputParser   
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough   
from langchain.utils.math import cosine_similarity   

physics_template = """"""You are a very smart physics professor. \  
You are great at answering questions about physics in a concise and easy to understand manner. \  
When you don't know the answer to a question you admit that you don't know.  

Here is a question:  
{query}""""""  

math_template = """"""You are a very good mathematician. You are great at answering math questions. \  
You are so good because you are able to break down hard problems into their component parts, \  
answer the component parts, and then put them together to answer the broader question.  

Here is a question:  
{query}""""""  

embeddings = OpenAIEmbeddings()  
prompt_templates = [physics_template, math_template]  
prompt_embeddings = embeddings.embed_documents(prompt_templates)  

def prompt_router(input):  
    query_embedding = embeddings.embed_query(input[""query""])  
    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]  
    most_similar = prompt_templates[similarity.argmax()]  
    print(""Using MATH"" if most_similar == math_template else ""Using PHYSICS"")  
    return PromptTemplate.from_template(most_similar)  

chain = (  
    {""query"": RunnablePassthrough()}  
    | RunnableLambda(prompt_router)  
    | ChatOpenAI()  
    | StrOutputParser()  
)  

print(chain.invoke(""What's a black hole""))  

    Using PHYSICS  
    A black hole is a region in space where gravity is extremely strong, so strong that nothing, not even light, can escape its gravitational pull. It is formed when a massive star collapses under its own gravity during a supernova explosion. The collapse causes an incredibly dense mass to be concentrated in a small volume, creating a gravitational field that is so intense that it warps space and time. Black holes have a boundary called the event horizon, which marks the point of no return for anything that gets too close. Beyond the event horizon, the gravitational pull is so strong that even light cannot escape, hence the name ""black hole."" While we have a good understanding of black holes, there is still much to learn, especially about what happens inside them.  

print(chain.invoke(""What's a path integral""))  

    Using MATH  
    Thank you for your kind words! I will do my best to break down the concept of a path integral for you.  
    ... (continues with the explanation of path integral)"
Как добавить память к произвольной цепочке?,"Пример добавления памяти к произвольной цепочке.

from operator import itemgetter   
from langchain.chat_models import ChatOpenAI   
from langchain.memory import ConversationBufferMemory   
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder   
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough   

model = ChatOpenAI()  
prompt = ChatPromptTemplate.from_messages([  
    (""system"", ""You are a helpful chatbot""),  
    MessagesPlaceholder(variable_name=""history""),  
    (""human"", ""{input}""),  
])  

memory = ConversationBufferMemory(return_messages=True)  

memory.load_memory_variables({})  
    {'history': []}  

chain = (  
    RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(""history""))  
    | prompt  
    | model  
)  

inputs = {""input"": ""hi im bob""}  
response = chain.invoke(inputs)  
    AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, example=False)  

memory.save_context(inputs, {""output"": response.content})  

memory.load_memory_variables({})  
    {'history': [HumanMessage(content='hi im bob', additional_kwargs={}, example=False),  
    AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, example=False)]}  

inputs = {""input"": ""whats my name""}  
response = chain.invoke(inputs)  
    AIMessage(content='Your name is Bob.', additional_kwargs={}, example=False)"
Как добавить модерацию (или другие меры безопасности) вокруг вашего приложения LLM?,"Пример добавления модерации вокруг приложения LLM.

from langchain.chains import OpenAIModerationChain   
from langchain.llms import OpenAI   
from langchain.prompts import ChatPromptTemplate   

moderate = OpenAIModerationChain()  

model = OpenAI()  
prompt = ChatPromptTemplate.from_messages([(""system"", ""repeat after me: {input}"")])  

chain = prompt | model   

chain.invoke({""input"": ""you are stupid""})  
    '

You are stupid.'  

moderated_chain = chain | moderate   

moderated_chain.invoke({""input"": ""you are stupid""})  
    {'input': '

You are stupid',  
     'output': ""Text was found that violates OpenAI's content policy.""}"
Как решить вопрос с управлением размером запроса без его обработки?,"Пример решения вопроса с управлением размером запроса без его обработки:

agent = (  
    {  
        ""input"": itemgetter(""input""),  
        ""agent_scratchpad"": lambda x: format_to_openai_function_messages(  
            x[""intermediate_steps""]  
        ),  
    }  
    | prompt  
    | llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])  
    | OpenAIFunctionsAgentOutputParser()  
)  

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)  
agent_executor.invoke(  
    {  
        ""input"": ""Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?""  
    }  
)"
Как легко использовать инструменты с Runnables?,"Пример использования инструментов с Runnables.

pip install duckduckgo-search   

from langchain.chat_models import ChatOpenAI   
from langchain.prompts import ChatPromptTemplate   
from langchain.schema.output_parser import StrOutputParser   
from langchain.tools import DuckDuckGoSearchRun   

search = DuckDuckGoSearchRun()  

template = """"""turn the following user input into a search query for a search engine:  

{input}""""""  
prompt = ChatPromptTemplate.from_template(template)  

model = ChatOpenAI()  

chain = prompt | model | StrOutputParser() | search   

chain.invoke({""input"": ""I'd like to figure out what games are tonight""})  

    'What sports games are on TV today & tonight? Watch and stream live sports on TV today, tonight, tomorrow. Today's 2023 sports TV schedule includes football, basketball, baseball, hockey, motorsports, soccer and more. Watch on TV or stream online on ESPN, FOX, FS1, CBS, NBC, ABC, Peacock, Paramount+, fuboTV, local channels and many other networks. MLB Games Tonight: How to Watch on TV, Streaming & Odds - Thursday, September 7. Seattle Mariners' Julio Rodriguez greets teammates in the dugout after scoring against the Oakland Athletics in a ... Circle - Country Music and Lifestyle. Live coverage of all the MLB action today is available to you, with the information provided below. The Brewers will look to pick up a road win at PNC Park against the Pirates on Wednesday at 12:35 PM ET. Check out the latest odds and with BetMGM Sportsbook. Use bonus code ""GNPLAY"" for special offers! MLB Games Tonight: How to Watch on TV, Streaming & Odds - Tuesday, September 5. Houston Astros' Kyle Tucker runs after hitting a double during the fourth inning of a baseball game against the Los Angeles Angels, Sunday, Aug. 13, 2023, in Houston. (AP Photo/Eric Christian Smith) (APMedia) The Houston Astros versus the Texas Rangers is one of ... The second half of tonight's college football schedule still has some good games remaining to watch on your television.. We've already seen an exciting one when Colorado upset TCU. And we saw some ...'"
Как использовать подключение к хранилищу функций в LangChain?,"Пример подключения к хранилищу функций в LangChain.

from feast import FeatureStore  
feast_repo_path = ""../../../../../my_feature_repo/feature_repo/""  
store = FeatureStore(repo_path=feast_repo_path)  

from langchain.prompts import PromptTemplate, StringPromptTemplate   

template = """"""Given the driver's up to date stats, write them note relaying those stats to them.  
If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better   

Here are the drivers stats:  
Conversation rate: {conv_rate}  
Acceptance rate: {acc_rate}  
Average Daily Trips: {avg_daily_trips}  

Your response:""""""  
prompt = PromptTemplate.from_template(template)  

class FeastPromptTemplate(StringPromptTemplate):  
    def format(self, **kwargs) -> str:  
        driver_id = kwargs.pop(""driver_id"")  
        feature_vector = store.get_online_features(  
            features=[  
                ""driver_hourly_stats:conv_rate"",  
                ""driver_hourly_stats:acc_rate"",  
                ""driver_hourly_stats:avg_daily_trips"",  
            ],  
            entity_rows=[{""driver_id"": driver_id}],  
        ).to_dict()  
        kwargs[""conv_rate""] = feature_vector[""conv_rate""][0]  
        kwargs[""acc_rate""] = feature_vector[""acc_rate""][0]  
        kwargs[""avg_daily_trips""] = feature_vector[""avg_daily_trips""][0]  
        return prompt.format(**kwargs)  

prompt_template = FeastPromptTemplate(input_variables=[""driver_id""])  

print(prompt_template.format(driver_id=1001))  
    'Given the driver's up to date stats, write them note relaying those stats to them. If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better Here are the drivers stats: Conversation rate: 0.4745151400566101 Acceptance rate: 0.055561766028404236 Average Daily Trips: 936 Your response:'"
Как создать и использовать пользовательский шаблон запроса в LangChain?,"import inspect   
from langchain.prompts import StringPromptTemplate   
from pydantic import BaseModel, validator   

PROMPT = """"""Given the function name and source code, generate an English language explanation of the function.
Function Name: {function_name}
Source Code:
{source_code}
Explanation:
""""""

class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):
    """"""A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.""""""

    @validator(""input_variables"")
    def validate_input_variables(cls, v):
        """"""Validate that the input variables are correct.""""""
        if len(v) != 1 or ""function_name"" not in v:
            raise ValueError(""function_name must be the only input_variable."")
        return v

    def format(self, **kwargs) -> str:
        # Get the source code of the function
        source_code = get_source_code(kwargs[""function_name""])
        # Generate the prompt to be sent to the language model
        prompt = PROMPT.format(function_name=kwargs[""function_name""].__name__, source_code=source_code)
        return prompt

    def _prompt_type(self):
        return ""function-explainer""

# Example of using the custom prompt template
fn_explainer = FunctionExplainerPromptTemplate(input_variables=[""function_name""])

# Generate a prompt for the function ""get_source_code""
prompt = fn_explainer.format(function_name=get_source_code)
print(prompt)"
Как использовать шаблоны запросов с несколькими примерами для моделей чата в LangChain?,"from langchain.prompts import (  
        ChatPromptTemplate,  
        FewShotChatMessagePromptTemplate,  
    )  

    examples = [  
        {""input"": ""2+2"", ""output"": ""4""},  
        {""input"": ""2+3"", ""output"": ""5""},  
    ]  

    example_prompt = ChatPromptTemplate.from_messages(  
        [  
            (""human"", ""{input}""),  
            (""ai"", ""{output}""),  
        ]  
    )  
    few_shot_prompt = FewShotChatMessagePromptTemplate(  
        example_prompt=example_prompt,  
        examples=examples,  
    )  

    final_prompt = ChatPromptTemplate.from_messages(  
        [  
            (""system"", ""You are a wondrous wizard of math.""),  
            few_shot_prompt,  
            (""human"", ""{input}""),  
        ]  
    )  

    from langchain.chat_models import ChatAnthropic   
    chain = final_prompt | ChatAnthropic(temperature=0.0)  
    chain.invoke({""input"": ""What's the square of a triangle?""})  
        AIMessage(content=' Triangles do not have a ""square"". A square refers to a shape with 4 equal sides and 4 right angles. Triangles have 3 sides and 3 angles.

The area of a triangle can be calculated using the formula:

A = 1/2 * b * h

Where:

A is the area 
b is the base (the length of one of the sides)
h is the height (the length from the base to the opposite vertex)

So the area depends on the specific dimensions of the triangle. There is no single ""square of a triangle"". The area can vary greatly depending on the base and height measurements.'"
Каковы различные способы форматирования вывода шаблона запросов в LangChain?,"Вывод метода format в LangChain может быть представлен как строка, список сообщений и `ChatPromptValue`. Строка представляет собой простой текстовый вывод, список сообщений включает в себя объекты сообщений с дополнительной информацией, а `ChatPromptValue` предоставляет более структурированный формат вывода, сочетающий несколько элементов."
Как форматировать запрос в LangChain для получения вывода в виде строки?,"Для форматирования запроса в LangChain и получения вывода в виде строки, необходимо использовать метод format или format_prompt, который возвращает строку с оформленным запросом, включающим входные и выходные данные."
Как можно получить вывод запроса в виде списка объектов сообщений в LangChain?,"В LangChain для получения вывода запроса в виде списка объектов сообщений используется метод format_prompt с последующим вызовом to_messages, который возвращает список объектов SystemMessage и HumanMessage, каждый из которых содержит свой контент и дополнительные параметры."
Какой метод в LangChain используется для форматирования вывода шаблона запроса?,"В LangChain метод `format` или `format_prompt` используется для форматирования вывода шаблона запроса. Эти методы возвращают строку, которая представляет собой оформленный запрос, готовый к отправке в модель языка."
Как в LangChain преобразовать форматированный запрос в список объектов сообщений?,"Для преобразования форматированного запроса в список объектов сообщений в LangChain, можно использовать метод `to_messages` после `format_prompt`. Это преобразует запрос в список, содержащий объекты `SystemMessage` и `HumanMessage`, каждый с собственным содержанием и параметрами."
Как использовать метод format для форматирования шаблона запроса в LangChain?,"# Пример кода для форматирования шаблона запроса в LangChain
from langchain.prompts import PromptTemplate

template = PromptTemplate(template_string=""Your prompt here {variable}"")
formatted_prompt = template.format(variable=""value"")
# formatted_prompt будет содержать строку ""Your prompt here value""
"
Как преобразовать форматированный шаблон запроса в список объектов сообщений в LangChain?,"# Пример кода для преобразования форматированного запроса в список сообщений
from langchain.prompts import PromptTemplate

template = PromptTemplate(template_string=""Your prompt here {variable}"")
formatted_prompt = template.format(variable=""value"")
message_list = formatted_prompt.to_messages()
# message_list будет содержать список объектов сообщений, основанный на formatted_prompt
"
В чем разница между использованием Jinja2 и f-string форматов в шаблонах запросов LangChain?,"Jinja2 и f-string форматы предоставляют разные способы вставки переменных в шаблоны запросов LangChain. Jinja2 использует синтаксис с двойными фигурными скобками (например, {{ variable }}), в то время как f-string использует фигурные скобки с прямым указанием переменной (например, {variable}). Выбор между ними зависит от предпочтений пользователя и требуемой гибкости в оформлении шаблонов."
Какие преимущества предлагает Jinja2 формат при создании шаблонов запросов?,"Jinja2 формат предлагает расширенную гибкость и мощные возможности для создания сложных шаблонов запросов. Он поддерживает условные конструкции, циклы и другие функции шаблонизации, что делает его идеальным для создания сложных или динамически меняющихся запросов в LangChain."
В каких случаях предпочтительнее использовать f-string формат для шаблонов запросов?,"F-string формат более прост и прямолинеен для использования, когда требуется только основная подстановка переменных без сложной логики оформления. Он идеально подходит для более простых или статических запросов, где нет необходимости в расширенной функциональности, предоставляемой Jinja2."
Как использовать ChatMessagePromptTemplate для создания пользовательского сообщения в LangChain?,"from langchain.prompts import ChatMessagePromptTemplate

prompt = ""May the {subject} be with you""
chat_message_prompt = ChatMessagePromptTemplate.from_template(role=""Jedi"", template=prompt)
chat_message_prompt.format(subject=""force"")
# Вывод: ChatMessage(content='May the force be with you', additional_kwargs={}, role='Jedi')"
Какие типы MessagePromptTemplate предоставляет LangChain и для чего они используются?,"LangChain предоставляет различные типы MessagePromptTemplate, включая AIMessagePromptTemplate, SystemMessagePromptTemplate и HumanMessagePromptTemplate, которые создают соответственно сообщение AI, системное сообщение и человеческое сообщение. Кроме того, есть ChatMessagePromptTemplate для создания чатовых сообщений с произвольной ролью, и MessagesPlaceholder для полного контроля над отображаемыми сообщениями."
Как использовать метод partial для создания частичного шаблона запроса в LangChain?,"from langchain.prompts import PromptTemplate   

prompt = PromptTemplate(template=""{foo}{bar}"", input_variables=[""foo"", ""bar""])
partial_prompt = prompt.partial(foo=""foo"")
partial_prompt.format(bar=""baz"")
# Вывод: foobaz"
Какие преимущества предоставляет использование частичных шаблонов запросов в LangChain?,"Использование частичных шаблонов запросов в LangChain позволяет заранее задать некоторые переменные в шаблоне, упрощая дальнейшее использование шаблона. Это особенно полезно, когда некоторые значения известны заранее или необходимо использовать функции для автоматического заполнения переменных, таких как текущая дата или время."
Как загружать шаблоны запросов из YAML или JSON в LangChain?,"from langchain.prompts import load_prompt

# Loading a prompt from YAML
prompt_yaml = load_prompt(""path/to/prompt.yaml"")

# Loading a prompt from JSON
prompt_json = load_prompt(""path/to/prompt.json"")
# В этих примерах 'prompt_yaml' и 'prompt_json' загружаются из соответствующих файлов и готовы к использованию."
Каковы преимущества сериализации шаблонов запросов в форматах YAML и JSON в LangChain?,"Сериализация шаблонов запросов в форматах YAML и JSON в LangChain позволяет легко делиться, хранить и управлять версиями шаблонов. Эти форматы удобочитаемы и поддерживаются широким кругом инструментов, облегчая интеграцию шаблонов в различные рабочие процессы и системы управления версиями."
Как соединять несколько шаблонов запросов вместе в LangChain?,"from langchain.prompts import PromptTemplate

# Combining string prompts
prompt = (
    PromptTemplate.from_template(""Tell me a joke about {topic}"")
    + "", make it funny""
    + ""\n\nand in {language}""
)
formatted_prompt = prompt.format(topic=""sports"", language=""spanish"")
# formatted_prompt будет содержать строку ""Tell me a joke about sports, make it funny

and in spanish""
"
В чем заключается идея 'pipelining' шаблонов запросов в LangChain?,"Идея 'pipelining' шаблонов запросов в LangChain заключается в предоставлении удобного интерфейса для компоновки различных частей шаблонов вместе. Это позволяет легко повторно использовать компоненты и создавать более сложные шаблоны запросов, сочетая их друг с другом."
Как реализовать пользовательский класс выбора примеров в LangChain?,"from langchain.prompts import BaseExampleSelector
from typing import List, Dict
from abc import ABC, abstractmethod

class CustomExampleSelector(BaseExampleSelector):
    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
        # Custom logic to select examples based on the inputs
        pass
# В этом примере 'CustomExampleSelector' является пользовательским классом для выбора примеров, основанный на входных переменных."
Для чего используется класс ExampleSelector в LangChain?,"Класс ExampleSelector в LangChain используется для выбора примеров, которые будут включены в запрос. Это особенно важно, когда у вас большое количество примеров, и вам нужно определить, какие из них наиболее релевантны для конкретного входа."
Как создать и использовать пользовательский класс выбора примеров в LangChain?,"from langchain.prompts.example_selector.base import BaseExampleSelector
from typing import Dict, List
import numpy as np

class CustomExampleSelector(BaseExampleSelector):
    def __init__(self, examples: List[Dict[str, str]]):
        self.examples = examples

    def add_example(self, example: Dict[str, str]) -> None:
        """"""Add new example to store for a key.""""""
        self.examples.append(example)

    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
        """"""Select which examples to use based on the inputs.""""""
        return np.random.choice(self.examples, size=2, replace=False)

# Usage example
examples = [{""foo"": ""1""}, {""foo"": ""2""}, {""foo"": ""3""}]
example_selector = CustomExampleSelector(examples)
selected_examples = example_selector.select_examples({""foo"": ""foo""})
# selected_examples contains 2 randomly chosen examples from the list"
Как работает пользовательский класс выбора примеров в LangChain?,"Пользовательский класс выбора примеров в LangChain позволяет разработчику определить свою логику для выбора примеров, которые будут использоваться в шаблоне запроса. Например, класс может выбирать примеры случайным образом или на основе определенных критериев, связанных с входными переменными."
Как использовать LengthBasedExampleSelector для выбора примеров по длине?,"from langchain.prompts import PromptTemplate, FewShotPromptTemplate
from langchain.prompts.example_selector import LengthBasedExampleSelector

examples = [{""input"": ""happy"", ""output"": ""sad""}, {""input"": ""big"", ""output"": ""small""}]
example_prompt = PromptTemplate(template=""Input: {input}\nOutput: {output}"")
example_selector = LengthBasedExampleSelector(
    examples=examples, example_prompt=example_prompt, max_length=25)

dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector, example_prompt=example_prompt,
    prefix=""Give the antonym of every input"", suffix=""Input: {adjective}\nOutput:"",
    input_variables=[""adjective""])

print(dynamic_prompt.format(adjective=""big""))
# Output includes examples that fit within the max_length limit."
В каких сценариях полезно использовать выбор примеров по длине в LangChain?,"Выбор примеров по длине в LangChain полезен, когда нужно учитывать ограничения по длине запроса, особенно для моделей языка с ограниченным размером контекстного окна. Это помогает убедиться, что включенные примеры не превышают максимально допустимую длину, что особенно важно при работе с длинными входными данными."
Как использовать MaxMarginalRelevanceExampleSelector для выбора примеров в LangChain?,"from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

example_prompt = PromptTemplate(
    input_variables=[""input"", ""output""],
    template=""Input: {input}\nOutput: {output}""
)
examples = [
    {""input"": ""happy"", ""output"": ""sad""},
    {""input"": ""big"", ""output"": ""small""}
]

example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    examples=examples,
    embedding=OpenAIEmbeddings(),
    vector_store=FAISS,
    k=2
)
mmr_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=""Provide the opposite of each input"",
    suffix=""Input: {word}\nOutput:"",
    input_variables=[""word""]
)

print(mmr_prompt.format(word=""large""))
# The output will include examples selected by MMR based on the input 'large'."
Каковы преимущества использования MMR для выбора примеров в LangChain?,"Использование MMR (Maximal Marginal Relevance) для выбора примеров в LangChain позволяет выбирать примеры, которые не только релевантны входным данным, но и обеспечивают разнообразие в ответах. Это помогает избежать повторения и увеличивает шансы на получение более информативного и разнообразного набора примеров."
Как использовать NGramOverlapExampleSelector для выбора примеров по совпадению N-грамм?,"from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector

example_prompt = PromptTemplate(
    input_variables=[""input"", ""output""],
    template=""Input: {input}\nOutput: {output}""
)
examples = [
    {""input"": ""See Spot run."", ""output"": ""Ver correr a Spot.""},
    {""input"": ""My dog barks."", ""output"": ""Mi perro ladra.""},
    {""input"": ""Spot can run."", ""output"": ""Spot puede correr.""}
]

example_selector = NGramOverlapExampleSelector(
    examples=examples,
    example_prompt=example_prompt,
    threshold=-1.0
)
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=""Give the Spanish translation of every input"",
    suffix=""Input: {sentence}\nOutput:"",
    input_variables=[""sentence""]
)

print(dynamic_prompt.format(sentence=""Spot can run fast.""))
# Output includes examples selected by N-gram overlap score."
Каковы преимущества использования выбора примеров по N-граммам в LangChain?,"Выбор примеров по N-граммам в LangChain позволяет выбирать примеры, наиболее похожие на входные данные, на основе совпадения N-грамм. Это увеличивает вероятность того, что выбранные примеры будут релевантными и помогут модели языка лучше понять контекст и дать более точный ответ."
Как использовать SemanticSimilarityExampleSelector для выбора примеров на основе семантической схожести?,"from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

example_prompt = PromptTemplate(
    input_variables=[""input"", ""output""],
    template=""Input: {input}\nOutput: {output}""
)
examples = [
    {""input"": ""happy"", ""output"": ""sad""},
    {""input"": ""tall"", ""output"": ""short""}
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples=examples,
    embedding=OpenAIEmbeddings(),
    vector_store=Chroma,
    k=1
)
similar_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=""Give the antonym of every input"",
    suffix=""Input: {adjective}\nOutput:"",
    input_variables=[""adjective""]
)

print(similar_prompt.format(adjective=""worried""))
# Output includes examples selected by semantic similarity."
В чем преимущества использования выбора примеров на основе семантической схожести в LangChain?,"Использование выбора примеров на основе семантической схожести в LangChain позволяет выбирать примеры, которые близки по смыслу к входным данным. Это помогает в создании более точных и релевантных запросов, так как выбранные примеры лучше отражают контекст входных данных."
Как использовать класс ChatOpenAI для создания чата с использованием модели OpenAI?,"from langchain.chat_models import ChatOpenAI

# Initialize the ChatOpenAI model
model = ChatOpenAI()

# Create a chat session with the model
response = model.chat(""What's the weather like today?"")
# response содержит ответ модели на заданный вопрос
"
Какие функции предлагает класс ChatOpenAI в LangChain?,"Класс ChatOpenAI в LangChain предоставляет функциональность для создания интерактивного чата с использованием моделей OpenAI. Он позволяет отправлять запросы, получать ответы и вести диалоговую сессию, что делает его полезным для создания чат-ботов или интерактивных систем на основе искусственного интеллекта."
Как настроить параметры для модели чата ChatOpenAI в LangChain?,"from langchain.chat_models import ChatOpenAI

# Initialize the ChatOpenAI model with specific parameters
model = ChatOpenAI(temperature=0.7, max_tokens=150)

# Create a chat session with the model using these parameters
response = model.chat(""Tell me a story about space."")
# response содержит ответ модели на запрос, сгенерированный с учетом заданных параметров"
"Какие особенности у моделей чата, таких как ChatOpenAI, делают их подходящими для интерактивных приложений?","Модели чата, такие как ChatOpenAI, обладают особенностями, такими как способность поддерживать контекст разговора, генерировать естественные и релевантные ответы, и настраивать параметры генерации (например, температура и максимальное количество токенов), что делает их идеальными для использования в интерактивных приложениях, чат-ботах и системах на основе AI."
Как использовать ChatPromptTemplate для создания пользовательского чата в LangChain?,"from langchain.prompts import PromptTemplate
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

template = ""You are a helpful assistant that translates {input_language} to {output_language}.""
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template = ""{text}""
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
formatted_messages = chat_prompt.format_prompt(input_language=""English"", output_language=""French"", text=""I love programming."").to_messages()

# Example of using the formatted messages for a chat session
chat_response = chat(formatted_messages)
# chat_response содержит ответ чат-модели на сформированные сообщения
"
Какие возможности предоставляют шаблоны сообщений в создании чатов в LangChain?,"Шаблоны сообщений в LangChain, такие как SystemMessagePromptTemplate и HumanMessagePromptTemplate, позволяют создавать разнообразные форматы сообщений для чатов. Они обеспечивают гибкость в оформлении диалогов, позволяя интегрировать различные типы сообщений (от системных до человеческих) и создавать более динамичные и интерактивные сессии чата."
Как создать и использовать ChatPromptTemplate в LangChain?,"from langchain.prompts import PromptTemplate
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# Define a system message prompt
template = ""You are a helpful assistant that translates {input_language} to {output_language}.""
system_message_prompt = SystemMessagePromptTemplate.from_template(template)

# Define a human message prompt
human_template = ""{text}""
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# Combine prompts to create a ChatPromptTemplate
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Use the chat prompt
chat_completion = chat_prompt.format_prompt(input_language=""English"", output_language=""French"", text=""I love programming."").to_messages()
# chat_completion will be formatted as a series of messages ready to be sent to a chat model
"
Каковы преимущества использования ChatPromptTemplate в LangChain для создания диалоговых систем?,"Использование ChatPromptTemplate в LangChain позволяет создавать сложные и гибкие диалоговые системы, сочетая различные типы сообщений, такие как системные, человеческие и AI-сообщения. Это обеспечивает более естественный и гибкий диалог с пользователем, а также позволяет настраивать ответы в зависимости от контекста разговора."
Как использовать различные типы MessagePromptTemplate для создания ChatPromptTemplate?,"from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# Define system and human message templates
system_message_prompt = SystemMessagePromptTemplate.from_template(""System message: {system_info}"")
human_message_prompt = HumanMessagePromptTemplate.from_template(""Human says: {human_text}"")

# Combine system and human messages into one chat prompt
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Example of using the combined chat prompt
formatted_prompt = chat_prompt.format_prompt(system_info=""Info"", human_text=""Hello!"")
# formatted_prompt contains both system and human messages integrated in one prompt
"
Как использовать потоковую передачу данных в чат-моделях LangChain?,"from langchain.chat_models import ChatAnthropic

# Initialize the ChatAnthropic model
chat = ChatAnthropic(model=""claude-2"")

# Use the stream method for streaming data
for chunk in chat.stream(""Write me a song about goldfish on the moon""):
    print(chunk.content, end="""", flush=True)
# This code will print each chunk of the response as it's received"
Каковы преимущества использования потоковой передачи в чат-моделях LangChain?,"Использование потоковой передачи в чат-моделях LangChain позволяет получать ответы по мере их генерации, вместо ожидания полного ответа. Это особенно полезно для длинных запросов или в сценариях, где немедленный отклик желателен, например, в интерактивных чат-ботах или приложениях для мгновенного общения."
В каких ситуациях особенно ценна потоковая передача в чат-моделях LangChain?,"Потоковая передача в чат-моделях LangChain особенно ценна в сценариях, требующих немедленного взаимодействия или постепенного предоставления информации, например, в интерактивных образовательных приложениях, играх или в приложениях для создания контента, где пользователь может видеть и взаимодействовать с частями ответа по мере их генерации."
Как отслеживать использование токенов в чат-моделях LangChain?,"from langchain.callbacks import get_openai_callback
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name=""gpt-4"")

with get_openai_callback() as cb:
    result = llm.invoke(""Tell me a joke"")
    print(cb)

# Вывод информации о использовании токенов, включая общее количество, токены запроса и токены завершения"
Каковы преимущества отслеживания использования токенов при работе с чат-моделями?,"Отслеживание использования токенов при работе с чат-моделями важно для оптимизации затрат и эффективности. Это позволяет пользователям понять, сколько токенов используется за запрос, что помогает в бюджетировании и анализе производительности чат-моделей."
Как использовать контекстный менеджер для отслеживания токенов в LangChain?,"from langchain.callbacks import get_openai_callback
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name=""gpt-4"")

with get_openai_callback() as cb:
    llm.invoke(""Tell me a joke"")
    llm.invoke(""What's the weather today?"")
    print(f""Total Tokens Used: {cb.total_tokens}"")

# Выводит общее количество использованных токенов за несколько вызовов"
Как инициализировать и использовать модель OpenAI LLM в LangChain?,"from langchain.llms import OpenAI

# Initialize the OpenAI LLM
llm = OpenAI(openai_api_key=""your-api-key"")

# Make a request to the LLM
response = llm.invoke(""Tell me a joke about programming."")
# response содержит ответ модели на запрос
"
Какие возможности предоставляют LLMs (Large Language Models) в LangChain?,"LLMs в LangChain предоставляют возможность взаимодействия с мощными языковыми моделями, такими как GPT от OpenAI. Они могут генерировать текст, отвечать на вопросы, анализировать текст и выполнять другие задачи, связанные с обработкой естественного языка, что делает их ценным инструментом для создания различных приложений AI."
Как использовать методы 'stream' и 'astream' с LLM в LangChain?,"from langchain.llms import OpenAI

# Initialize the OpenAI LLM
llm = OpenAI()

# Use the 'stream' method for streaming responses
for chunk in llm.stream(""What are the latest advancements in AI?""):
    print(chunk.content, end="""", flush=True)

# Alternatively, using 'astream' for asynchronous streaming
async for chunk in llm.astream(""Tell me about the history of the internet""):
    print(chunk.content, end="""", flush=True)
# These methods stream the response from the LLM in chunks, allowing real-time interaction.
"
Как использовать асинхронный вызов с LLM в LangChain?,"import asyncio
from langchain.llms import OpenAI

async def async_invoke():
    llm = OpenAI()
    response = await llm.ainvoke(""What is the capital of France?"")
    print(response)

# Running the asynchronous function
asyncio.run(async_invoke())
# This function asynchronously invokes the LLM and prints the response
"
Каковы преимущества использования асинхронных вызовов с LLM в LangChain?,"Использование асинхронных вызовов с LLM в LangChain позволяет улучшить производительность за счет одновременного выполнения нескольких задач. Это особенно полезно при работе с множественными запросами или в сценариях, где важна немедленная обработка запросов без блокирования основного потока выполнения."
Как выполнить несколько асинхронных запросов одновременно в LangChain?,"import asyncio
from langchain.llms import OpenAI

async def async_invoke(llm, query):
    return await llm.ainvoke(query)

async def multiple_async_invokes():
    llm = OpenAI()
    tasks = [async_invoke(llm, ""Tell me a joke""), async_invoke(llm, ""What's the weather like?"")]
    responses = await asyncio.gather(*tasks)
    print(responses)

# Running multiple asynchronous invokes
asyncio.run(multiple_async_invokes())
# This function runs multiple asynchronous invokes simultaneously
"
Как создать пользовательский класс LLM в LangChain?,"from langchain.llms.base import LLM

class CustomLLM(LLM):
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:
        # Custom logic for the LLM
        pass

# Example usage
custom_llm = CustomLLM()
response = custom_llm.invoke(""Generate a story about a dragon."")
# response содержит ответ, сгенерированный пользовательским LLM
"
Каковы преимущества создания пользовательского класса LLM в LangChain?,"Создание пользовательского класса LLM в LangChain позволяет интегрировать любую модель обработки естественного языка, не ограничиваясь предустановленными вариантами. Это обеспечивает гибкость и позволяет пользователям использовать специализированные модели, наилучшим образом соответствующие их потребностям."
Как определить идентифицирующие параметры для пользовательского LLM в LangChain?,"from langchain.llms.base import LLM

class CustomLLM(LLM):
    def __init__(self, special_parameter: int):
        self.special_parameter = special_parameter

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {""special_parameter"": self.special_parameter}

    def _call(self, prompt: str, **kwargs: Any) -> str:
        # Custom logic for the LLM
        pass

# Example usage
custom_llm = CustomLLM(special_parameter=42)
print(custom_llm)
# Outputs the custom LLM with its identifying parameters"
Как настроить кэширование для модели LLM в LangChain?,"from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain.llms import OpenAI

# Set up in-memory caching for LLMs
set_llm_cache(InMemoryCache())

# Initialize the OpenAI LLM with caching
llm = OpenAI(model_name=""text-davinci-002"")

# Making a prediction with caching
response1 = llm.predict(""Tell me a joke"")
response2 = llm.predict(""Tell me a joke"")
# The second prediction will be faster as it is retrieved from cache"
Какие преимущества предоставляет кэширование в моделях LLM в LangChain?,"Кэширование в моделях LLM в LangChain позволяет ускорить время ответа и снизить количество запросов к API, сохраняя результаты предыдущих вызовов. Это уменьшает нагрузку на модель и сокращает расходы на использование API, делая процесс более эффективным и экономичным."
Как использовать кэширование SQLite для LLM в LangChain?,"from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache
from langchain.llms import OpenAI

# Set up SQLite caching for LLMs
set_llm_cache(SQLiteCache(database_path="".langchain.db""))

# Initialize the OpenAI LLM with SQLite caching
llm = OpenAI(model_name=""text-davinci-002"")

# Making a prediction with SQLite caching
response = llm.predict(""Tell me a joke"")
# The response will be cached in the SQLite database for future use"
Как сериализовать и десериализовать объект LLM в LangChain?,"from langchain.llms import OpenAI
from langchain.load import dumpd, dumps, load, loads

# Initialize an OpenAI LLM object
llm = OpenAI(model=""gpt-3.5-turbo-instruct"")

# Serialize the LLM object
serialized_llm_dict = dumpd(llm)
serialized_llm_str = dumps(llm)

# Deserialize the LLM object
deserialized_llm_dict = load(serialized_llm_dict)
deserialized_llm_str = loads(serialized_llm_str)
# These operations convert the LLM object to and from serializable formats"
Каковы преимущества сериализации LLM в LangChain?,"Сериализация LLM в LangChain позволяет сохранять состояние модели и ее параметры в формате, который можно легко передавать, хранить и восстанавливать. Это особенно полезно для сохранения конфигураций модели, обеспечивая удобство и согласованность при повторном использовании или развертывании модели."
"Как проверить, является ли объект LLM сериализуемым в LangChain?","from langchain.llms import OpenAI

# Initialize an OpenAI LLM object
llm = OpenAI(model=""gpt-3.5-turbo-instruct"")

# Check if the LLM object is serializable
is_serializable = OpenAI.is_lc_serializable()
# 'is_serializable' will be True if the LLM object can be serialized"
Как реализовать потоковую передачу данных в LLM с использованием LangChain?,"from langchain.llms import OpenAI

llm = OpenAI(model=""gpt-3.5-turbo-instruct"", temperature=0, max_tokens=512)

# Using the stream method for real-time data streaming
for chunk in llm.stream(""Write me a song about sparkling water.""):
    print(chunk, end="""", flush=True)
# This code streams the response from the LLM, printing each chunk as it's received."
В чем заключаются преимущества потоковой передачи данных в LLM с использованием LangChain?,"Потоковая передача данных в LLM с использованием LangChain позволяет получать и обрабатывать ответы по мере их поступления, что ускоряет общение и обработку данных. Это особенно полезно для динамических приложений, где важно получать информацию в режиме реального времени."
Как обрабатывать асинхронно получаемые данные при потоковой передаче в LLM?,"import asyncio
from langchain.llms import OpenAI

async def stream_response(llm, prompt):
    async for chunk in llm.astream(prompt):
        print(chunk, end="""", flush=True)

llm = OpenAI(model=""gpt-3.5-turbo-instruct"")
asyncio.run(stream_response(llm, ""Tell me about the history of the internet""))
# This function asynchronously streams the response, allowing for real-time interaction."
Как отслеживать использование токенов для отдельных вызовов LLM в LangChain?,"from langchain.callbacks import get_openai_callback
from langchain.llms import OpenAI

llm = OpenAI(model_name=""gpt-3.5-turbo-instruct"")

with get_openai_callback() as cb:
    result = llm.invoke(""Tell me a joke"")
    print(cb)

# Выводит информацию о количестве использованных токенов для данного запроса"
Каковы преимущества отслеживания использования токенов в LangChain?,"Отслеживание использования токенов в LangChain помогает в анализе затрат и оптимизации использования моделей языкового AI. Это позволяет пользователям понять, какие запросы потребляют больше ресурсов и как можно улучшить эффективность их использования."
Как отслеживать общее использование токенов за несколько запросов в LangChain?,"from langchain.callbacks import get_openai_callback
from langchain.llms import OpenAI

llm = OpenAI()

with get_openai_callback() as cb:
    llm.invoke(""Tell me a joke"")
    llm.invoke(""What's the weather today?"")
    print(f""Total Tokens Used: {cb.total_tokens}"")

# Выводит общее количество использованных токенов за несколько запросов
"
Как использовать PydanticOutputParser для структурирования ответов LLM в LangChain?,"from langchain.llms import OpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field

class Joke(BaseModel):
    setup: str = Field(description=""Setup of the joke"")
    punchline: str = Field(description=""Punchline of the joke"")

model = OpenAI(model_name=""text-davinci-003"", temperature=0.0)
parser = PydanticOutputParser(Joke)
prompt = PromptTemplate(template=""Tell me a joke.
Setup: {setup}
Punchline: {punchline}"")
chain = prompt | model | parser

output = chain.invoke({""setup"": ""Why did the chicken cross the road?""})
print(output)
# This code structures the response from the LLM into a Pydantic model"
Какие преимущества предоставляет использование PydanticOutputParser в LangChain?,"PydanticOutputParser в LangChain позволяет преобразовать текстовые ответы от LLM в структурированные, типизированные объекты Python. Это обеспечивает более удобную работу с данными, лучшую проверку и валидацию ответов, а также упрощает интеграцию с другими системами."
Как использовать CommaSeparatedListOutputParser для обработки выходных данных в LangChain?,"from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template=""List five {subject}.
{format_instructions}"",
    input_variables=[""subject""],
    partial_variables={""format_instructions"": format_instructions}
)

model = OpenAI(temperature=0)
_input = prompt.format(subject=""ice cream flavors"")
output = model(_input)
parsed_output = output_parser.parse(output)
# parsed_output содержит список элементов, разделенных запятыми"
В каких сценариях полезно использовать CommaSeparatedListOutputParser в LangChain?,"CommaSeparatedListOutputParser в LangChain полезен в сценариях, где требуется преобразовать текстовый ответ в список элементов, разделенных запятыми. Это особенно ценно при обработке ответов, содержащих перечисления или списки, например, список ингредиентов, имен или других подобных данных."
Как CommaSeparatedListOutputParser облегчает обработку сложных ответов в LangChain?,"CommaSeparatedListOutputParser в LangChain облегчает обработку сложных ответов, автоматически преобразуя длинные текстовые строки, содержащие перечисления, в структурированные списки. Это упрощает последующую обработку и анализ данных, представляя их в более удобном и доступном формате."
Как использовать DatetimeOutputParser для анализа выходных данных в формате даты и времени в LangChain?,"from langchain.chains import LLMChain   
from langchain.llms import OpenAI   
from langchain.output_parsers import DatetimeOutputParser   
from langchain.prompts import PromptTemplate   

output_parser = DatetimeOutputParser()  
template = """"""Answer the user's question:  

{question}  

{format_instructions}""""""  
prompt = PromptTemplate.from_template(  
    template,  
    partial_variables={""format_instructions"": output_parser.get_format_instructions()},  
)  

chain = LLMChain(prompt=prompt, llm=OpenAI())  

output = chain.run(""around when was bitcoin founded?"")  
parsed_output = output_parser.parse(output)
# parsed_output будет содержать дату и время в формате datetime.datetime
"
В каких случаях полезно использовать DatetimeOutputParser в LangChain?,"DatetimeOutputParser в LangChain полезен в ситуациях, когда выходные данные модели языка необходимо преобразовать в точный формат даты и времени. Это может быть важно для приложений, требующих обработки или сравнения дат, например, в календарных ботах, системах бронирования или аналитических приложениях."
Как DatetimeOutputParser облегчает обработку и анализ временных данных в LangChain?,"DatetimeOutputParser в LangChain облегчает обработку и анализ временных данных, автоматически преобразуя текстовые ответы в структурированный формат datetime. Это позволяет интегрировать и сравнивать временные данные с другими системами и приложениями, упрощая задачи, связанные с планированием, аналитикой и отчетностью."
Как использовать EnumOutputParser для анализа выходных данных с перечислениями в LangChain?,"from langchain.output_parsers.enum import EnumOutputParser
from enum import Enum

class Colors(Enum):
    RED = ""red""
    GREEN = ""green""
    BLUE = ""blue""

parser = EnumOutputParser(enum=Colors)

# Examples of parsing
parsed_red = parser.parse(""red"")  # Outputs <Colors.RED: 'red'>
parsed_green = parser.parse("" green"")  # Outputs <Colors.GREEN: 'green'>
parsed_blue = parser.parse(""blue\n"")  # Outputs <Colors.BLUE: 'blue'>
"
В каких сценариях полезно использовать EnumOutputParser в LangChain?,"EnumOutputParser в LangChain полезен в сценариях, где выходные данные модели языка представляют собой значения из определенного перечня или перечисления. Это может быть использовано в приложениях для классификации ответов, фильтрации по определенным категориям или для обеспечения строгого соответствия выходных данных определенным стандартам."
Как EnumOutputParser облегчает обработку и использование перечисленных данных в LangChain?,"EnumOutputParser в LangChain облегчает обработку и использование перечисленных данных, преобразуя текстовые ответы в значения конкретного Enum класса. Это обеспечивает более структурированное и точное представление данных, упрощая их интеграцию в различные бизнес-процессы и приложения."
Как использовать OutputFixingParser для исправления ошибок форматирования в выходных данных LLM?,"from langchain.output_parsers import OutputFixingParser, PydanticOutputParser
from langchain.chat_models import ChatOpenAI
from langchain.pydantic_v1 import BaseModel, Field
from typing import List

class Actor(BaseModel):
    name: str = Field(description=""name of an actor"")
    film_names: List[str] = Field(description=""list of films they starred in"")

parser = PydanticOutputParser(pydantic_object=Actor)
new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())
misformatted = ""{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}""
new_parser.parse(misformatted)
# Исправляет форматирование и возвращает объект Actor"
В каких случаях полезно использовать OutputFixingParser в LangChain?,"OutputFixingParser в LangChain полезен в случаях, когда выходные данные LLM содержат ошибки форматирования или не соответствуют ожидаемому структурированному формату. Этот парсер автоматически исправляет ошибки, облегчая интеграцию данных в последующие процессы или системы."
Как OutputFixingParser улучшает обработку данных от LLM в LangChain?,"OutputFixingParser в LangChain улучшает обработку данных от LLM, предоставляя механизм для автоматического исправления ошибок форматирования. Это обеспечивает более высокую точность и надежность данных, полученных от модели, и упрощает их использование в различных приложениях и аналитических инструментах."
Как использовать PydanticOutputParser для анализа структурированных JSON ответов в LangChain?,"from langchain.llms import OpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import BaseModel, Field

class Joke(BaseModel):
    setup: str = Field(description=""question to set up a joke"")
    punchline: str = Field(description=""answer to resolve the joke"")

model_name = ""text-davinci-003""
temperature = 0.0
model = OpenAI(model_name=model_name, temperature=temperature)

parser = PydanticOutputParser(pydantic_object=Joke)
prompt = PromptTemplate(
    template=""Answer the user query.
{format_instructions}
{query}
"",
    input_variables=[""query""],
    partial_variables={""format_instructions"": parser.get_format_instructions()}
)
_input = prompt.format_prompt(query=""Tell me a joke."")
output = model(_input.to_string())
parsed_output = parser.parse(output)
# parsed_output содержит объект Joke со значениями 'setup' и 'punchline'"
"Как использовать PydanticOutputParser с моделью Pydantic, содержащей список?","from langchain.llms import OpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import BaseModel, Field
from typing import List

class Actor(BaseModel):
    name: str = Field(description=""name of an actor"")
    film_names: List[str] = Field(description=""list of names of films they starred in"")

model_name = ""text-davinci-003""
model = OpenAI(model_name=model_name, temperature=0.0)
parser = PydanticOutputParser(pydantic_object=Actor)
prompt = PromptTemplate(
    template=""Answer the user query.
{format_instructions}
{query}
"",
    input_variables=[""query""],
    partial_variables={""format_instructions"": parser.get_format_instructions()}
)
_input = prompt.format_prompt(query=""Generate the filmography for a random actor."")
output = model(_input.to_string())
parsed_output = parser.parse(output)
# parsed_output содержит объект Actor с полями 'name' и 'film_names'"
В каких случаях полезно использовать PydanticOutputParser в LangChain?,"PydanticOutputParser в LangChain полезен в сценариях, где выходные данные модели языка должны соответствовать специфическому структурированному JSON формату. Это может быть использовано для анализа сложных данных, таких как формы, списки или любые другие структурированные ответы, требующие строгого соответствия определенным структурам данных."
Как использовать RetryWithErrorOutputParser для повторного запроса с исправлением ошибок в LangChain?,"from langchain.output_parsers import RetryWithErrorOutputParser, PydanticOutputParser
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field

class Action(BaseModel):
    action: str = Field(description=""action to take"")
    action_input: str = Field(description=""input to the action"")

parser = PydanticOutputParser(pydantic_object=Action)
retry_parser = RetryWithErrorOutputParser.from_llm(
    parser=parser, llm=OpenAI(temperature=0)
)

prompt_template = ""Answer the user query.
{format_instructions}
{query}
""
prompt = PromptTemplate.from_template(
    prompt_template,
    input_variables=[""query""],
    partial_variables={""format_instructions"": parser.get_format_instructions()},
)

prompt_value = prompt.format_prompt(query=""who is leo di caprios gf?"")
bad_response = '{""action"": ""search""}'
corrected_output = retry_parser.parse_with_prompt(bad_response, prompt_value)
# corrected_output содержит исправленный ответ с учетом формата Action"
В каких ситуациях полезно использовать RetryWithErrorOutputParser в LangChain?,"RetryWithErrorOutputParser в LangChain полезен в ситуациях, когда первоначальный ответ модели содержит ошибки или неполные данные. Парсер позволяет повторно запросить ответ, устраняя ошибки или заполняя отсутствующую информацию, что улучшает точность и полноту выходных данных."
Как RetryWithErrorOutputParser улучшает качество выходных данных LLM в LangChain?,"RetryWithErrorOutputParser в LangChain улучшает качество выходных данных LLM, позволяя автоматически исправлять ошибки и дополнять неполные ответы. Это делает процесс обработки данных более надежным и эффективным, снижая вероятность ошибок и недоразумений в результатах."
Как использовать StructuredOutputParser для форматирования структурированных ответов в LangChain?,"from langchain.output_parsers import StructuredOutputParser, ResponseSchema   
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate   
from langchain.llms import OpenAI   
from langchain.chat_models import ChatOpenAI   

response_schemas = [  
    ResponseSchema(name=""answer"", description=""answer to the user's question""),  
    ResponseSchema(name=""source"", description=""source used to answer the user's question, should be a website."")  
]  
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)  

format_instructions = output_parser.get_format_instructions()  
prompt = PromptTemplate(  
    template=""answer the users question as best as possible.
{format_instructions}
{question}"",  
    input_variables=[""question""],  
    partial_variables={""format_instructions"": format_instructions}  
)  

model = OpenAI(temperature=0)  
_input = prompt.format_prompt(question=""what's the capital of france?"")  
output = model(_input.to_string())  
parsed_output = output_parser.parse(output)
# parsed_output возвращает словарь с ключами 'answer' и 'source'"
В каких сценариях полезно использовать StructuredOutputParser в LangChain?,"StructuredOutputParser в LangChain полезен в сценариях, где необходимо преобразовать ответы моделей языка в структурированный формат с несколькими полями. Это может быть полезно в приложениях, требующих детализации ответов, таких как каталогизация информации, сбор данных из различных источников или ответы с подробными ссылками."
Как StructuredOutputParser улучшает обработку сложных ответов в LangChain?,"StructuredOutputParser улучшает обработку сложных ответов в LangChain, предоставляя возможность структурировать ответы в формат с определенными полями. Это облегчает интерпретацию и использование данных, обеспечивая более точное извлечение информации и ее представление в понятном виде."
Как использовать XMLOutputParser для обработки XML-ответов модели в LangChain?,"from langchain.llms import Anthropic
from langchain.output_parsers import XMLOutputParser
from langchain.prompts import PromptTemplate

model = Anthropic(model=""claude-2"", max_tokens_to_sample=512, temperature=0.1)
parser = XMLOutputParser()
prompt = PromptTemplate(
    template=""""""Human: {query} {format_instructions} Assistant:"""""",
    input_variables=[""query""],
    partial_variables={""format_instructions"": parser.get_format_instructions()}
)

chain = prompt | model | parser
actor_query = ""Generate the shortened filmography for Tom Hanks.""
output = chain.invoke({""query"": actor_query})
print(output)
# Пример вывода: {'filmography': [{'movie': [{'title': 'Splash'}, {'year': '1984'}]}, ... ]}"
Как использовать XMLOutputParser с кастомными тегами в LangChain?,"from langchain.llms import Anthropic
from langchain.output_parsers import XMLOutputParser
from langchain.prompts import PromptTemplate

parser = XMLOutputParser(tags=[""movies"", ""actor"", ""film"", ""name"", ""genre""])
prompt = PromptTemplate(
    template=""""""Human: {query} {format_instructions} Assistant:"""""",
    input_variables=[""query""],
    partial_variables={""format_instructions"": parser.get_format_instructions()}
)

chain = prompt | model | parser
actor_query = ""Generate the shortened filmography for Tom Hanks.""
output = chain.invoke({""query"": actor_query})
print(output)
# Пример вывода: {'movies': [{'actor': [{'name': 'Tom Hanks'}, {'film': [{'name': 'Splash'}, {'genre': 'Comedy'}]}, ... ]}
"
В каких случаях полезно использовать XMLOutputParser в LangChain?,"XMLOutputParser в LangChain полезен в ситуациях, когда необходимо преобразовать текстовые ответы моделей языка в структурированный XML-формат. Это особенно актуально для задач, связанных с извлечением данных из текста, их структурированием и подготовкой к дальнейшей обработке или визуализации."
Что такое модуль 'Data Connection' в LangChain?,"Модуль 'Data Connection' в LangChain предоставляет функциональность для подключения к различным источникам данных и их использования в контексте LLM (Large Language Models). Это включает в себя инструменты для загрузки документов, их преобразования, создания векторных представлений и организации эффективного поиска и извлечения информации."
Какие основные компоненты входят в модуль 'Data Connection' LangChain?,"Модуль 'Data Connection' LangChain включает в себя: 'Document Loaders' для загрузки документов из различных источников, 'Document Transformers' для преобразования документов, 'Text Embedding Models' для создания семантических представлений, 'Vector Stores' для хранения и поиска эмбеддингов, 'Retrievers' для извлечения данных и 'Indexing' для синхронизации данных с векторными хранилищами."
Какие преимущества предоставляет интеграция с 'Data Connection' в LangChain?,"Интеграция с 'Data Connection' в LangChain позволяет расширить возможности LLM, интегрируя пользовательские данные и контекст в процесс генерации ответов. Это улучшает релевантность и точность ответов, позволяя моделям языка адаптироваться к конкретным потребностям пользователя и обеспечивая более глубокое понимание контекста запросов."
Как использовать TextLoader для загрузки документов в LangChain?,"from langchain.document_loaders import TextLoader

loader = TextLoader(""./index.md"")
documents = loader.load()
# 'documents' содержит список загруженных документов из файла 'index.md'
"
Для чего используются Document Loaders в LangChain?,"Document Loaders в LangChain используются для загрузки данных из различных источников как 'Document', то есть как кусок текста с ассоциированными метаданными. Они могут загружать текст из файлов, веб-страниц, транскриптов YouTube видео и других источников, обеспечивая гибкость в работе с различными типами данных."
Какие типы Document Loaders доступны в LangChain?,"В LangChain доступны различные типы Document Loaders, включая загрузчики для текстовых файлов, веб-страниц, CSV-файлов, PDF-документов, HTML-контента и многих других форматов. Это позволяет пользователям выбирать подходящий загрузчик в зависимости от формата и источника данных, которые им необходимо обработать."
Как использовать CSVLoader для загрузки данных из CSV-файла в LangChain?,"from langchain.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv')
data = loader.load()
# 'data' содержит список документов, загруженных из указанного CSV-файла"
Как настроить CSVLoader для использования кастомных параметров парсинга CSV в LangChain?,"from langchain.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv', csv_args={
    'delimiter': ',',
    'quotechar': '""',
    'fieldnames': ['MLB Team', 'Payroll in millions', 'Wins']
})
data = loader.load()
# 'data' содержит список документов с кастомными параметрами парсинга"
Какие преимущества предоставляет использование CSVLoader в LangChain?,"CSVLoader в LangChain позволяет эффективно загружать и обрабатывать данные из CSV-файлов, преобразуя их в документы для дальнейшего использования в LLM. Это облегчает интеграцию и анализ табличных данных, которые часто используются в различных бизнес- и исследовательских приложениях."
Как использовать DirectoryLoader для загрузки документов из директории в LangChain?,"from langchain.document_loaders import DirectoryLoader

loader = DirectoryLoader('../', glob=""**/*.md"")
docs = loader.load()
# 'docs' содержит список загруженных документов из указанной директории с расширением '.md'
"
Как настроить DirectoryLoader для загрузки с отображением прогресса в LangChain?,"from langchain.document_loaders import DirectoryLoader

loader = DirectoryLoader('../', glob=""**/*.md"", show_progress=True)
docs = loader.load()
# 'docs' содержит загруженные документы с отображением прогресса загрузки
"
В каких случаях полезно использовать DirectoryLoader в LangChain?,"DirectoryLoader в LangChain полезен для загрузки всех документов из указанной директории, что позволяет автоматически обрабатывать большое количество файлов. Это особенно важно в сценариях, где требуется извлекать или анализировать данные из множества документов, расположенных в одном месте."
Как использовать UnstructuredHTMLLoader для загрузки HTML-документов в LangChain?,"from langchain.document_loaders import UnstructuredHTMLLoader

loader = UnstructuredHTMLLoader(""example_data/fake-content.html"")
data = loader.load()
# 'data' содержит список документов, загруженных из HTML-файла 'fake-content.html'
"
Как использовать BSHTMLLoader для загрузки и анализа HTML-документов с BeautifulSoup в LangChain?,"from langchain.document_loaders import BSHTMLLoader

loader = BSHTMLLoader(""example_data/fake-content.html"")
data = loader.load()
# 'data' содержит список документов с извлеченным текстом и метаданными из HTML-файла 'fake-content.html'
"
В каких случаях полезно использовать загрузчики HTML-документов в LangChain?,"Загрузчики HTML-документов в LangChain полезны при работе с веб-страницами или другими HTML-источниками данных. Они позволяют извлекать текстовое содержимое и метаданные для дальнейшего использования в приложениях, связанных с анализом данных, поисковыми системами или контент-менеджментом."
Как использовать JSONLoader для загрузки JSON-документов в LangChain?,"from langchain.document_loaders import JSONLoader

loader = JSONLoader(
    file_path='./example_data/facebook_chat.json',
    jq_schema='.messages[].content',
    text_content=False
)
data = loader.load()
# 'data' содержит список документов, загруженных из файла JSON 'facebook_chat.json'
"
Как использовать JSONLoader для загрузки документов из файла JSON Lines в LangChain?,"from langchain.document_loaders import JSONLoader

loader = JSONLoader(
    file_path='./example_data/facebook_chat_messages.jsonl',
    jq_schema='.content',
    text_content=False,
    json_lines=True
)
data = loader.load()
# 'data' содержит список документов, загруженных из файла JSON Lines 'facebook_chat_messages.jsonl'
"
Каковы преимущества использования JSONLoader в LangChain для обработки JSON-данных?,"JSONLoader в LangChain обеспечивает гибкое и мощное средство для загрузки и обработки данных из JSON-файлов, включая поддержку сложных структур данных и возможность настройки схемы извлечения. Это позволяет эффективно интегрировать и использовать JSON-данные в различных приложениях на основе LangChain."
Как использовать UnstructuredMarkdownLoader для загрузки документов Markdown в LangChain?,"from langchain.document_loaders import UnstructuredMarkdownLoader

loader = UnstructuredMarkdownLoader(""../../../../../README.md"")
data = loader.load()
# 'data' содержит список документов, загруженных из файла Markdown 'README.md'
"
Как использовать UnstructuredMarkdownLoader с режимом 'elements' для загрузки элементов Markdown?,"from langchain.document_loaders import UnstructuredMarkdownLoader

loader = UnstructuredMarkdownLoader(""../../../../../README.md"", mode=""elements"")
data = loader.load()
# 'data' содержит список элементов, извлеченных из файла Markdown 'README.md'
"
В каких случаях полезно использовать загрузчик Markdown в LangChain?,"Загрузчик Markdown в LangChain полезен при работе с документацией, текстами, блогами и другими данными, представленными в формате Markdown. Он позволяет извлекать текстовое содержимое и метаданные для дальнейшего использования в приложениях, связанных с анализом данных, генерацией контента или обучением."
Как использовать PyPDFLoader для загрузки PDF-документов в LangChain?,"from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader(""example_data/layout-parser-paper.pdf"")
pages = loader.load_and_split()
# 'pages' содержит список документов, каждый из которых представляет собой отдельную страницу PDF-документа 'layout-parser-paper.pdf'
"
Как использовать MathpixPDFLoader для загрузки и обработки PDF-документов в LangChain?,"from langchain.document_loaders import MathpixPDFLoader

loader = MathpixPDFLoader(""example_data/layout-parser-paper.pdf"")
data = loader.load()
# 'data' содержит список документов, загруженных из PDF-файла 'layout-parser-paper.pdf', с применением OCR
"
В каких случаях полезно использовать загрузчики PDF-документов в LangChain?,"Загрузчики PDF-документов в LangChain полезны при работе с PDF-файлами, содержащими текстовую или графическую информацию. Они позволяют извлекать текст и обрабатывать изображения из PDF для дальнейшего использования в приложениях анализа данных, текстовой обработки или для обучения моделей."
Что такое 'Document Transformers' в LangChain?,"Document Transformers' в LangChain - это инструменты, предназначенные для преобразования загруженных документов для улучшения их совместимости с LLM. Они включают функции для разделения текста на части, фильтрации избыточных документов, перевода документов, извлечения метаданных и преобразования диалогов в формат вопросов и ответов."
Какие типы преобразований документов доступны в LangChain?,"В LangChain доступны различные типы преобразований документов, включая: 'Text Splitters' для разделения длинных текстов на управляемые части, 'Filter Redundant Docs' для фильтрации похожих документов, 'Translate Docs' для перевода документов, 'Extract Metadata' для добавления метаданных к документам и 'Convert Conversational Dialogue' для преобразования диалогов в формат вопросов и ответов."
Как использовать HTMLHeaderTextSplitter для разделения текста HTML на заголовки и ассоциированный контент?,"from langchain.text_splitter import HTMLHeaderTextSplitter   

html_string = """"""  
<!DOCTYPE html>  
<html>  
<body>  
    <div>  
        <h1>Foo</h1>  
        <p>Some intro text about Foo.</p>  
        <div>  
            <h2>Bar main section</h2>  
            <p>Some intro text about Bar.</p>  
            <h3>Bar subsection 1</h3>  
            <p>Some text about the first subtopic of Bar.</p>  
            <h3>Bar subsection 2</h3>  
            <p>Some text about the second subtopic of Bar.</p>  
        </div>  
        <div>  
            <h2>Baz</h2>  
            <p>Some text about Baz</p>  
        </div>  
        <br>  
        <p>Some concluding text about Foo</p>  
    </div>  
</body>  
</html>  
""""""  

headers_to_split_on = [  
    (""h1"", ""Header 1""),  
    (""h2"", ""Header 2""),  
    (""h3"", ""Header 3""),  
]  

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)  
html_header_splits = html_splitter.split_text(html_string)
# html_header_splits содержит разделенные документы с соответствующими заголовками и контентом"
"Как использовать HTMLHeaderTextSplitter для разделения текста HTML, загруженного из URL?","from langchain.text_splitter import HTMLHeaderTextSplitter   

url = ""https://plato.stanford.edu/entries/goedel/""  
headers_to_split_on = [(""h1"", ""Header 1""), (""h2"", ""Header 2""), (""h3"", ""Header 3""), (""h4"", ""Header 4"")]  
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)  

# Для локального файла используйте html_splitter.split_text_from_file(<path_to_file>)
html_header_splits = html_splitter.split_text_from_url(url)
# html_header_splits содержит разделенные документы с заголовками и контентом, загруженными из указанного URL"
В каких сценариях полезно использовать HTMLHeaderTextSplitter в LangChain?,"HTMLHeaderTextSplitter в LangChain полезен при обработке HTML-документов, где важно сохранить структуру документа, разделяя текст на основе заголовков. Это может быть полезно в аналитических приложениях, системах контент-менеджмента, или в задачах обработки естественного языка, где контекст и структура документа играют ключевую роль."
Как использовать CharacterTextSplitter для разделения текста на части по символам в LangChain?,"from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator = ""\n\n"",
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
    is_separator_regex = False,
)

# Example text
state_of_the_union = ""Long document text...""

texts = text_splitter.create_documents([state_of_the_union])
# 'texts' содержит список документов, разделенных на части размером 1000 символов с перекрытием в 200 символов"
Как использовать CharacterTextSplitter для разделения текста с метаданными в LangChain?,"from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator = ""\n\n"",
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
    is_separator_regex = False,
)

metadatas = [{""document"": 1}, {""document"": 2}]
documents = text_splitter.create_documents([state_of_the_union, state_of_the_union], metadatas=metadatas)
# 'documents' содержит список документов с метаданными, разделенных на части размером 1000 символов"
В каких случаях полезно использовать CharacterTextSplitter в LangChain?,"CharacterTextSplitter в LangChain полезен при работе с длинными текстами, где требуется разделить текст на меньшие части для улучшения обработки и анализа. Это может быть полезно для обработки длинных документов, статей или транскриптов, где каждая часть текста должна быть независимо обработана или анализирована."
Как использовать CodeTextSplitter для разделения кода на Python в LangChain?,"from langchain.text_splitter import CodeTextSplitter, Language

python_splitter = CodeTextSplitter(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=100
)

python_code = '''
def example_function():
    print(""This is an example."")
    # Additional code
'''

python_docs = python_splitter.create_documents([python_code])
# 'python_docs' содержит список документов, разделенных на части размером 1000 символов с перекрытием в 100 символов"
Как использовать CodeTextSplitter для разделения кода на JavaScript в LangChain?,"from langchain.text_splitter import CodeTextSplitter, Language

js_splitter = CodeTextSplitter(
    language=Language.JS,
    chunk_size=500,
    chunk_overlap=50
)

javascript_code = '''
function exampleFunction() {
    console.log(""This is an example."");
    // Additional code
}
'''

js_docs = js_splitter.create_documents([javascript_code])
# 'js_docs' содержит список документов, разделенных на части размером 500 символов с перекрытием в 50 символов"
В каких сценариях полезно использовать CodeTextSplitter в LangChain?,"CodeTextSplitter в LangChain полезен при работе с кодом на различных языках программирования, где необходимо разделить длинные скрипты на более мелкие части для улучшения обработки. Это может быть полезно для анализа кода, автоматической документации или обучения моделей на основе фрагментов кода."
Как использовать MarkdownHeaderTextSplitter для разделения текста Markdown на заголовки и ассоциированный контент?,"from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_document = """"""# Foo\n\n ## Bar\n\nHi this is Jim  \nHi this is Joe\n\n ## Baz\n\n Hi this is Molly""""""
headers_to_split_on = [(""#"", ""Header 1""),(""##"", ""Header 2"")]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
md_header_splits = markdown_splitter.split_text(markdown_document)
# md_header_splits содержит разделенные документы с соответствующими заголовками и контентом"
Как применять MarkdownHeaderTextSplitter к содержимому Markdown с множественными уровнями заголовков?,"from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_document = """"""# Intro\n\n ## History\n\n Markdown is a lightweight markup language...\n\n ## Implementations\n\n Implementations of Markdown...""""""
headers_to_split_on = [(""#"", ""Header 1""),(""##"", ""Header 2""),(""###"", ""Header 3"")]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
md_header_splits = markdown_splitter.split_text(markdown_document)
# md_header_splits содержит разделенные документы с уровнями заголовков и соответствующим контентом"
В каких сценариях полезно использовать MarkdownHeaderTextSplitter в LangChain?,"MarkdownHeaderTextSplitter в LangChain полезен при обработке Markdown-документов, где важно сохранить структуру документа, разделяя текст на основе заголовков. Это может быть полезно для аналитических приложений, систем контент-менеджмента или для задач обработки естественного языка, где контекст и структура документа играют ключевую роль."
Как использовать RecursiveCharacterTextSplitter для рекурсивного разделения длинных текстов в LangChain?,"from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
    is_separator_regex = False,
)

# Example text
state_of_the_union = ""Long document text...""

texts = text_splitter.create_documents([state_of_the_union])
# 'texts' содержит список документов, рекурсивно разделенных на части размером 100 символов с перекрытием в 20 символов"
Как использовать RecursiveCharacterTextSplitter для разделения текста с учетом различных разделителей в LangChain?,"from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    separators=[""\n\n"", ""\n"", "" ""],
    chunk_size = 500,
    chunk_overlap  = 50,
    length_function = len,
    is_separator_regex = False,
)

long_text = ""Text with multiple paragraphs, lines, and spaces...""

texts = text_splitter.create_documents([long_text])
# 'texts' содержит список документов, разделенных на основе заданных разделителей"
В каких случаях полезно использовать RecursiveCharacterTextSplitter в LangChain?,"RecursiveCharacterTextSplitter в LangChain полезен при обработке длинных текстов, где требуется рекурсивно разделить текст на меньшие части для улучшения обработки и анализа. Это может быть полезно для обработки длинных документов, статей или транскриптов, где каждая часть текста должна быть независимо обработана или анализирована."
Как использовать TokenTextSplitter для разделения текста на токены в LangChain?,"from langchain.text_splitter import TokenTextSplitter
from transformers import GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained(""gpt2"")
text_splitter = TokenTextSplitter(
    tokenizer,
    chunk_size=100,
    chunk_overlap=0
)

state_of_the_union = ""Long document text...""
texts = text_splitter.split_text(state_of_the_union)
# 'texts' содержит список документов, разделенных на токены размером 100 с перекрытием в 0"
Как использовать TokenTextSplitter с другим токенизатором для разделения текста?,"from langchain.text_splitter import TokenTextSplitter
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained(""roberta-base"")
text_splitter = TokenTextSplitter(
    tokenizer,
    chunk_size=200,
    chunk_overlap=20
)

state_of_the_union = ""Another long document text...""
texts = text_splitter.split_text(state_of_the_union)
# 'texts' содержит список документов, разделенных на токены размером 200 с перекрытием в 20"
В каких сценариях полезно использовать TokenTextSplitter в LangChain?,"TokenTextSplitter в LangChain полезен при работе с длинными текстами, где требуется разделить текст на токены для улучшения обработки и анализа. Это особенно полезно для языковых моделей с ограничением на количество токенов, где каждый фрагмент текста должен быть обработан отдельно."
Как использовать LongContextReorder для переупорядочивания документов в LangChain?,"from langchain.document_transformers import LongContextReorder

# Example list of documents
docs = [
    Document(page_content='The Celtics are my favourite team.', metadata={}),
    Document(page_content='The Boston Celtics won the game by 20 points', metadata={}),
    # Additional documents...
]

# Reorder the documents: Less relevant document will be at the middle of the list and more
# relevant elements at beginning / end.
reordering = LongContextReorder()
reordered_docs = reordering.transform_documents(docs)
# reordered_docs содержит список переупорядоченных документов"
Каковы преимущества использования LongContextReorder в LangChain?,"Преимущества использования LongContextReorder в LangChain включают улучшение производительности и точности моделей языка при работе с длинными контекстами, путем размещения наиболее релевантных документов в начале и в конце списка. Это помогает модели более эффективно находить и использовать важную информацию."
Как использовать OpenAIEmbeddings для встраивания документов в LangChain?,"from langchain.embeddings import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents([""Hi there!"", ""Oh, hello!"", ""What's your name?""])
# 'embeddings' содержит список векторных представлений для каждого текста
"
Как встраивать отдельные запросы с использованием OpenAIEmbeddings?,"from langchain.embeddings import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()
embedded_query = embeddings_model.embed_query(""What was the name mentioned in the conversation?"")
# 'embedded_query' содержит векторное представление для данного запроса
"
Какие преимущества предоставляют модели встраивания текста в LangChain?,"Модели встраивания текста в LangChain обеспечивают создание векторных представлений текстов, что позволяет выполнять семантический поиск и сравнивать тексты в векторном пространстве. Это улучшает обработку и анализ естественного языка, обеспечивая эффективную работу с большими объемами текстовых данных."
Как использовать OpenAIEmbeddings для встраивания списка текстов в LangChain?,"from langchain.embeddings import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings(openai_api_key=""your-api-key"")

embeddings = embeddings_model.embed_documents([
    ""Hi there!"",
    ""Oh, hello!"",
    ""What's your name?"",
    ""My friends call me World"",
    ""Hello World!""
])
# 'embeddings' содержит список векторных представлений каждого текста
"
Как использовать OpenAIEmbeddings для встраивания одиночного запроса в LangChain?,"from langchain.embeddings import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()

embedded_query = embeddings_model.embed_query(""What was the name mentioned in the conversation?"")
# 'embedded_query' содержит векторное представление для заданного запроса
"
Каковы преимущества использования моделей встраивания текста в LangChain?,"Модели встраивания текста в LangChain позволяют преобразовывать тексты в векторные представления, что упрощает их сравнение, анализ и поиск. Это особенно полезно для задач семантического поиска, кластеризации текста и анализа схожести текстов."
Как использовать Chroma для создания векторного хранилища в LangChain?,"from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma

raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = Chroma.from_documents(documents, OpenAIEmbeddings())
# 'db' содержит векторное хранилище Chroma, созданное из документов"
Как осуществить поиск похожих документов с использованием векторного хранилища в LangChain?,"from langchain.vectorstores import Chroma

query = ""What did the president say about Ketanji Brown Jackson""
docs = db.similarity_search(query)
print(docs[0].page_content)
# Выводит содержимое документа, наиболее похожего на заданный запрос"
Какие преимущества предоставляют векторные хранилища в LangChain?,"Векторные хранилища в LangChain предоставляют возможность хранения и быстрого поиска по семантически близким документам, используя векторное представление текста. Это полезно для реализации функций поиска, рекомендаций и анализа больших объемов текстовых данных."
Что такое 'retriever' в контексте LangChain?,"Retriever - это интерфейс, который возвращает документы, полученные в ответ на неструктурированный запрос. Это более общее понятие, чем хранилище векторов. Retriever не обязательно должен иметь возможность хранить документы, только возвращать (или извлекать) их. Хранилища векторов могут использоваться в качестве основы для retriever, но существуют и другие типы retriever'ов&#8203;``【oaicite:4】``&#8203;."
Какие методы поддерживаются интерфейсом 'retriever' в LangChain?,"Retriever'ы реализуют интерфейс Runnable, базовый строительный блок языка выражений LangChain (LCEL). Это означает, что они поддерживают вызовы `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log`&#8203;``【oaicite:3】``&#8203;."
Какой тип входных и выходных данных используется retriever'ом в LangChain?,Retriever принимает строковый запрос в качестве входных данных и возвращает список `Document` в качестве выходных данных&#8203;``【oaicite:2】``&#8203;.
Как настроить retriever с использованием Chroma vector store в LangChain?,"Чтобы настроить retriever с использованием Chroma vector store, необходимо выполнить `pip install chromadb` и загрузить файл `state_of_the_union.txt`. Затем используются модули OpenAIEmbeddings, CharacterTextSplitter и Chroma из langchain для создания и использования retriever'а&#8203;``【oaicite:1】``&#8203;."
Как комбинировать retriever с другими объектами Runnable в LangChain?,"Retriever'ы, будучи объектами Runnable, могут легко комбинироваться с другими объектами Runnable. Пример комбинации включает использование ChatOpenAI, ChatPromptTemplate, StrOutputParser и RunnablePassthrough из модуля langchain для создания цепочки обработки запросов&#8203;``【oaicite:0】``&#8203;."
Как установить Chroma vector store-backed retriever в LangChain?,"pip install chromadb   

    from langchain.embeddings import OpenAIEmbeddings   
    from langchain.text_splitter import CharacterTextSplitter   
    from langchain.vectorstores import Chroma   

    full_text = open(""state_of_the_union.txt"", ""r"").read()  
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)  
    texts = text_splitter.split_text(full_text)  

    embeddings = OpenAIEmbeddings()  
    db = Chroma.from_texts(texts, embeddings)  
    retriever = db.as_retriever()  

    retrieved_docs = retriever.invoke(  
        ""What did the president say about Ketanji Brown Jackson?""  
    )  
    print(retrieved_docs[0].page_content)"
Как скомбинировать retriever с другими Runnable объектами в LangChain?,"
    from langchain.chat_models import ChatOpenAI   
    from langchain.prompts import ChatPromptTemplate   
    from langchain.schema import StrOutputParser   
    from langchain.schema.runnable import RunnablePassthrough   

    template = ""Answer the question based only on the following context:  

    {context}  

    Question: {question}  
    ""  
    prompt = ChatPromptTemplate.from_template(template)  
    model = ChatOpenAI()  

    def format_docs(docs):  
        return ""

"".join([d.page_content for d in docs])  

    chain = (  
        {""context"": retriever | format_docs, ""question"": RunnablePassthrough()}  
        | prompt  
        | model  
        | StrOutputParser()  
    )  

    chain.invoke(""What did the president say about technology?"")  
    "
Что такое MultiQueryRetriever в LangChain и как он работает?,"MultiQueryRetriever автоматизирует процесс настройки запросов, используя LLM для генерации множества запросов из разных перспектив для заданного пользовательского входного запроса. Для каждого запроса он извлекает набор релевантных документов и берет уникальное объединение по всем запросам, чтобы получить больший набор потенциально релевантных документов&#8203;``【oaicite:4】``&#8203;."
Как построить пример vectorDB для использования с MultiQueryRetriever в LangChain?,"from langchain.document_loaders import WebBaseLoader   
    from langchain.embeddings.openai import OpenAIEmbeddings   
    from langchain.text_splitter import RecursiveCharacterTextSplitter   
    from langchain.vectorstores import Chroma  

    loader = WebBaseLoader(""https://lilianweng.github.io/posts/2023-06-23-agent/"")  
    data = loader.load()  

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)  
    splits = text_splitter.split_documents(data)  

    embedding = OpenAIEmbeddings()  
    vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
Как использовать MultiQueryRetriever с LLM для генерации запросов в LangChain?,"from langchain.chat_models import ChatOpenAI   
    from langchain.retrievers.multi_query import MultiQueryRetriever   

    question = ""What are the approaches to Task Decomposition?""  
    llm = ChatOpenAI(temperature=0)  
    retriever_from_llm = MultiQueryRetriever.from_llm(  
        retriever=vectordb.as_retriever(), llm=llm  
    )  

    import logging   

    logging.basicConfig()  
    logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO)  

    unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
Как можно предоставить свой запрос для MultiQueryRetriever в LangChain?,Для MultiQueryRetriever можно также предоставить собственный запрос вместе с парсером вывода для разделения результатов на список запросов&#8203;``【oaicite:1】``&#8203;.
Как настроить MultiQueryRetriever с пользовательским запросом и парсером вывода в LangChain?,"from typing import List   
    from langchain.chains import LLMChain   
    from langchain.output_parsers import PydanticOutputParser   
    from langchain.prompts import PromptTemplate   
    from pydantic import BaseModel, Field  

    class LineList(BaseModel):  
        lines: List[str] = Field(description=""Lines of text"")  

    class LineListOutputParser(PydanticOutputParser):  
        def __init__(self) -> None:  
            super().__init__(pydantic_object=LineList)  

        def parse(self, text: str) -> LineList:  
            lines = text.strip().split(""
"")  
            return LineList(lines=lines)  

    output_parser = LineListOutputParser()  

    QUERY_PROMPT = PromptTemplate(  
        input_variables=[""question""],  
        template=""You are an AI language model assistant. Your task is to generate five   
        different versions of the given user question to retrieve relevant documents from a vector   
        database. Provide these alternative questions separated by newlines.  
        Original question: {question}""  
    )  
    llm = ChatOpenAI(temperature=0)  

    llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)  

    retriever = MultiQueryRetriever(  
        retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=""lines""  
    )  

    unique_docs = retriever.get_relevant_documents(query=""What does the course say about regression?"")"
Что такое контекстуальное сжатие в LangChain и зачем оно нужно?,"Контекстуальное сжатие в LangChain предназначено для устранения проблемы извлечения документов, содержащих много нерелевантного текста. Вместо того, чтобы сразу возвращать извлеченные документы как есть, вы можете сжать их, используя контекст заданного запроса, чтобы возвращалась только релевантная информация&#8203;``【oaicite:5】``&#8203;."
Какие компоненты необходимы для использования контекстуального сжатия в LangChain?,"Для использования контекстуального сжатия необходим базовый retriever и компрессор документов. Контекстуальный компрессор отправляет запросы базовому retriever, получает начальные документы и передает их через компрессор документов, который сокращает список документов, уменьшая содержимое документов или полностью исключая некоторые документы&#8203;``【oaicite:4】``&#8203;."
Как инициализировать простой retriever хранилища векторов в LangChain?,"from langchain.text_splitter import CharacterTextSplitter   
    from langchain.embeddings import OpenAIEmbeddings   
    from langchain.document_loaders import TextLoader   
    from langchain.vectorstores import FAISS   

    documents = TextLoader('../../../state_of_the_union.txt').load()  
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)  
    texts = text_splitter.split_documents(documents)  
    retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()  

    docs = retriever.get_relevant_documents(""What did the president say about Ketanji Brown Jackson"")  
    pretty_print_docs(docs)"
Как добавить контекстуальное сжатие с помощью `LLMChainExtractor` в LangChain?,"from langchain.llms import OpenAI   
    from langchain.retrievers import ContextualCompressionRetriever   
    from langchain.retrievers.document_compressors import LLMChainExtractor   

    llm = OpenAI(temperature=0)  
    compressor = LLMChainExtractor.from_llm(llm)  
    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)  

    compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"")  
    pretty_print_docs(compressed_docs)"
Как работает `LLMChainFilter` в контекстуальном сжатии LangChain?,"
    from langchain.retrievers.document_compressors import LLMChainFilter   

    _filter = LLMChainFilter.from_llm(llm)  
    compression_retriever = ContextualCompressionRetriever(base_compressor=_filter, base_retriever=retriever)  

    compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"")  
    pretty_print_docs(compressed_docs)  
    "
Как использовать `EmbeddingsFilter` для контекстуального сжатия в LangChain?,"from langchain.embeddings import OpenAIEmbeddings   
    from langchain.retrievers.document_compressors import EmbeddingsFilter   

    embeddings = OpenAIEmbeddings()  
    embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)  
    compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)  

    compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"")  
    pretty_print_docs(compressed_docs)"
Что такое `EnsembleRetriever` в LangChain и как он работает?,"EnsembleRetriever в LangChain принимает список retriever'ов в качестве входных данных и объединяет результаты их методов `get_relevant_documents()`, переранжируя результаты на основе алгоритма Reciprocal Rank Fusion. EnsembleRetriever может достичь лучшей производительности, чем любой одиночный алгоритм, используя сильные стороны различных алгоритмов. Часто используется комбинация разреженного retriever'а (например, BM25) и плотного retriever'а (например, по семантическому сходству), поскольку их сильные стороны дополняют друг друга&#8203;``【oaicite:1】``&#8203;."
Как настроить и использовать `EnsembleRetriever` в LangChain?,"from langchain.retrievers import BM25Retriever, EnsembleRetriever   
    from langchain.vectorstores import FAISS   

    doc_list = [  
        ""I like apples"",  
        ""I like oranges"",  
        ""Apples and oranges are fruits"",  
    ]  

    bm25_retriever = BM25Retriever.from_texts(doc_list)  
    bm25_retriever.k = 2   

    embedding = OpenAIEmbeddings()  
    faiss_vectorstore = FAISS.from_texts(doc_list, embedding)  
    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={""k"": 2})  

    ensemble_retriever = EnsembleRetriever(  
        retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]  
    )  

    docs = ensemble_retriever.get_relevant_documents(""apples"")"
Что такое MultiVectorRetriever в LangChain и для чего он используется?,"MultiVectorRetriever в LangChain используется для хранения нескольких векторов для каждого документа. Это полезно во многих случаях, поскольку LangChain облегчает запросы в такой конфигурации. Сложность заключается в создании нескольких векторов для каждого документа, и в этом модуле описываются общие способы создания этих векторов и использования MultiVectorRetriever&#8203;``【oaicite:3】``&#8203;."
Как загрузить и разделить документы для использования в MultiVectorRetriever в LangChain?,"from langchain.retrievers.multi_vector import MultiVectorRetriever   

    from langchain.document_loaders import TextLoader   
    from langchain.embeddings import OpenAIEmbeddings   
    from langchain.storage import InMemoryStore   
    from langchain.text_splitter import RecursiveCharacterTextSplitter   
    from langchain.vectorstores import Chroma   

    loaders = [  
        TextLoader(""../../paul_graham_essay.txt""),  
        TextLoader(""../../state_of_the_union.txt""),  
    ]  
    docs = []  
    for l in loaders:  
        docs.extend(l.load())  
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)  
    docs = text_splitter.split_documents(docs)"
Как использовать MultiVectorRetriever с меньшими фрагментами документов в LangChain?,"vectorstore = Chroma(  
        collection_name=""full_documents"", embedding_function=OpenAIEmbeddings()  
    )  
    store = InMemoryStore()  
    id_key = ""doc_id""  
    retriever = MultiVectorRetriever(  
        vectorstore=vectorstore,  
        docstore=store,  
        id_key=id_key,  
    )  
    import uuid   

    doc_ids = [str(uuid.uuid4()) for _ in docs]  

    child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)  

    sub_docs = []  
    for i, doc in enumerate(docs):  
        _id = doc_ids[i]  
        _sub_docs = child_text_splitter.split_documents([doc])  
        for _doc in _sub_docs:  
            _doc.metadata[id_key] = _id  
        sub_docs.extend(_sub_docs)  

    retriever.vectorstore.add_documents(sub_docs)  
    retriever.docstore.mset(list(zip(doc_ids, docs)))"
Как создать и использовать резюме документов в MultiVectorRetriever в LangChain?,"import uuid   

    from langchain.chat_models import ChatOpenAI   
    from langchain.prompts import ChatPromptTemplate   
    from langchain.schema.document import Document   
    from langchain.schema.output_parser import StrOutputParser   

    chain = (  
        {""doc"": lambda x: x.page_content}  
        | ChatPromptTemplate.from_template(""Summarize the following document:

{doc}"")  
        | ChatOpenAI(max_retries=0)  
        | StrOutputParser()  
    )  

    summaries = chain.batch(docs, {""max_concurrency"": 5})  

    vectorstore = Chroma(collection_name=""summaries"", embedding_function=OpenAIEmbeddings())  
    store = InMemoryStore()  
    id_key = ""doc_id""  
    retriever = MultiVectorRetriever(  
        vectorstore=vectorstore,  
        docstore=store,  
        id_key=id_key,  
    )  
    doc_ids = [str(uuid.uuid4()) for _ in docs]  

    summary_docs = [  
        Document(page_content=s, metadata={id_key: doc_ids[i]})  
        for i, s in enumerate(summaries)  
    ]  

    retriever.vectorstore.add_documents(summary_docs)  
    retriever.docstore.mset(list(zip(doc_ids, docs)))"
Что такое ParentDocumentRetriever в LangChain и как он решает проблему разделения документов?,"ParentDocumentRetriever в LangChain находит баланс между желанием иметь маленькие документы, чтобы их векторные представления наиболее точно отражали их смысл, и необходимостью сохранения контекста каждого фрагмента. При извлечении он сначала получает маленькие фрагменты, а затем ищет родительские идентификаторы для этих фрагментов и возвращает эти более крупные документы&#8203;``【oaicite:2】``&#8203;."
Как настроить и использовать ParentDocumentRetriever для извлечения полных документов в LangChain?,"from langchain.retrievers import ParentDocumentRetriever   
    from langchain.document_loaders import TextLoader   
    from langchain.embeddings import OpenAIEmbeddings   
    from langchain.storage import InMemoryStore   
    from langchain.text_splitter import RecursiveCharacterTextSplitter   
    from langchain.vectorstores import Chroma   

    loaders = [TextLoader(""../../paul_graham_essay.txt""), TextLoader(""../../state_of_the_union.txt"")]  
    docs = []  
    for l in loaders:  
        docs.extend(l.load())  

    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)  
    vectorstore = Chroma(collection_name=""full_documents"", embedding_function=OpenAIEmbeddings())  
    store = InMemoryStore()  
    retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)  

    retriever.add_documents(docs, ids=None)"
Как настроить ParentDocumentRetriever для извлечения более крупных фрагментов в LangChain?,"parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)  
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)  
    vectorstore = Chroma(collection_name=""split_parents"", embedding_function=OpenAIEmbeddings())  
    store = InMemoryStore()  

    retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter)  

    retriever.add_documents(docs)  

    len(list(store.yield_keys()))  # Должно быть более двух документов, так как это более крупные фрагменты  

    sub_docs = vectorstore.similarity_search(""justice breyer"")  
    retrieved_docs = retriever.get_relevant_documents(""justice breyer"")  

    len(retrieved_docs[0].page_content)  # Длина содержимого извлеченных документов"
Что такое Self-querying Retriever в LangChain?,"Self-querying Retriever - это функция или компонент в LangChain, о котором упоминается в документации. Однако на данной странице отсутствует подробная информация или примеры кода, касающиеся этого компонента&#8203;``【oaicite:1】``&#8203;."
Где можно найти дополнительную информацию о работе Self-querying Retriever в LangChain?,"Для получения дополнительной информации о работе Self-querying Retriever в LangChain рекомендуется обратиться к другим разделам документации LangChain или источникам, предоставленным на странице, так как непосредственно в данном разделе подробные сведения отсутствуют&#8203;``【oaicite:0】``&#8203;."
Что делает Time-weighted Vector Store Retriever в LangChain?,"Time-weighted Vector Store Retriever в LangChain использует комбинацию семантического сходства и временного убывания (decay). Он оценивает документы, используя формулу `semantic_similarity + (1.0 - decay_rate) ^ hours_passed`, где `hours_passed` относится к часам, прошедшим с момента последнего доступа к объекту, а не с момента его создания&#8203;``【oaicite:3】``&#8203;."
Как настроить Time-weighted Vector Store Retriever с низким коэффициентом убывания в LangChain?,"import faiss   
    from datetime import datetime, timedelta   
    from langchain.docstore import InMemoryDocstore   
    from langchain.embeddings import OpenAIEmbeddings   
    from langchain.retrievers import TimeWeightedVectorStoreRetriever   
    from langchain.schema import Document   
    from langchain.vectorstores import FAISS  

    embeddings_model = OpenAIEmbeddings()  
    embedding_size = 1536   
    index = faiss.IndexFlatL2(embedding_size)  
    vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})  
    retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.0000000000000000000000001, k=1)  

    yesterday = datetime.now() - timedelta(days=1)  
    retriever.add_documents([Document(page_content=""hello world"", metadata={""last_accessed_at"": yesterday})])"
Как настроить Time-weighted Vector Store Retriever с высоким коэффициентом убывания в LangChain?,"embeddings_model = OpenAIEmbeddings()  
    embedding_size = 1536   
    index = faiss.IndexFlatL2(embedding_size)  
    vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})  
    retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.999, k=1)  

    yesterday = datetime.now() - timedelta(days=1)  
    retriever.add_documents([Document(page_content=""hello world"", metadata={""last_accessed_at"": yesterday})])"
Как использовать утилиты LangChain для имитации временного компонента в Time-weighted Vector Store Retriever?,"from langchain.utils import mock_now   
    import datetime  

    with mock_now(datetime.datetime(2011, 2, 3, 10, 11)):  
        print(retriever.get_relevant_documents(""hello world""))"
Что такое Vector Store-Backed Retriever в LangChain?,"Vector Store-Backed Retriever в LangChain - это ретривер, который использует векторное хранилище для извлечения документов. Это легковесная обертка вокруг класса векторного хранилища, которая позволяет ему соответствовать интерфейсу ретривера. Он использует методы поиска, реализованные векторным хранилищем, такие как поиск по схожести и MMR, для запроса текстов в векторном хранилище&#8203;``【oaicite:4】``&#8203;."
Как настроить Vector Store-Backed Retriever в LangChain?,"from langchain.document_loaders import TextLoader   
    loader = TextLoader('../../../state_of_the_union.txt')  
    from langchain.text_splitter import CharacterTextSplitter   
    from langchain.vectorstores import FAISS   
    from langchain.embeddings import OpenAIEmbeddings   

    documents = loader.load()  
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)  
    texts = text_splitter.split_documents(documents)  
    embeddings = OpenAIEmbeddings()  
    db = FAISS.from_documents(texts, embeddings)  
    retriever = db.as_retriever()"
Как использовать максимально значимый метод извлечения в Vector Store-Backed Retriever в LangChain?,"
    retriever = db.as_retriever(search_type=""mmr"")  
    "
Как настроить порог схожести для извлечения в Vector Store-Backed Retriever в LangChain?,"retriever = db.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .5})"
Как указать количество возвращаемых документов при извлечении в Vector Store-Backed Retriever в LangChain?,"retriever = db.as_retriever(search_kwargs={""k"": 1})"
Что делает WebResearchRetriever в LangChain?,"WebResearchRetriever в LangChain формулирует набор поисковых запросов в Google по заданному вопросу, осуществляет поиск по каждому из них, загружает все полученные URL-адреса, а затем выполняет встраивание и поиск по схожести с запросом на объединенном содержимом страниц&#8203;``【oaicite:2】``&#8203;."
Как настроить WebResearchRetriever для простого использования в LangChain?,"import os   
    from langchain.chat_models.openai import ChatOpenAI   
    from langchain.embeddings import OpenAIEmbeddings   
    from langchain.utilities import GoogleSearchAPIWrapper   
    from langchain.vectorstores import Chroma  

    vectorstore = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=""./chroma_db_oai"")  
    llm = ChatOpenAI(temperature=0)  

    os.environ[""GOOGLE_CSE_ID""] = ""xxx""  
    os.environ[""GOOGLE_API_KEY""] = ""xxx""  
    search = GoogleSearchAPIWrapper()  

    web_research_retriever = WebResearchRetriever.from_llm(vectorstore=vectorstore, llm=llm, search=search)"
Как использовать метод `get_relevant_documents` в WebResearchRetriever в LangChain?,"
    import logging   

    logging.basicConfig()  
    logging.getLogger(""langchain.retrievers.web_research"").setLevel(logging.INFO)  
    user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?""  
    docs = web_research_retriever.get_relevant_documents(user_input)  
    "
Что такое индексирование в LangChain и как оно работает?,"Индексирование в LangChain - это процесс загрузки и синхронизации документов из любого источника в векторное хранилище. Оно помогает избежать записи дублирующего контента в векторное хранилище, перезаписи неизмененного контента и повторного вычисления векторных представлений для неизмененного контента, что экономит время и улучшает результаты векторного поиска&#8203;``【oaicite:4】``&#8203;."
Как работает механизм индексирования в LangChain?,"LangChain использует менеджер записей (RecordManager), который отслеживает запись документов в векторное хранилище. При индексировании контента вычисляются хэши каждого документа, и в менеджере записей сохраняется информация о хеше документа (содержание страницы и метаданные), времени записи и идентификаторе источника&#8203;``【oaicite:3】``&#8203;."
Каковы режимы удаления при индексировании в LangChain?,"В LangChain существуют три режима удаления при индексировании: 'None', 'incremental' и 'full'. Режим 'None' не выполняет автоматическую очистку старого контента, в то время как 'incremental' и 'full' предлагают автоматическую очистку. Если содержание исходного документа или производных документов изменилось, оба режима 'incremental' и 'full' будут удалять предыдущие версии контента&#8203;``【oaicite:2】``&#8203;."
Как начать работу с индексированием в LangChain?,"from langchain.embeddings import OpenAIEmbeddings   
    from langchain.indexes import SQLRecordManager, index   
    from langchain.schema import Document   
    from langchain.vectorstores import ElasticsearchStore   

    collection_name = ""test_index""  
    embedding = OpenAIEmbeddings()  
    vectorstore = ElasticsearchStore(es_url=""http://localhost:9200"", index_name=""test_index"", embedding=embedding)"
Как использовать загрузчики с функцией индексирования в LangChain?,"from langchain.document_loaders.base import BaseLoader   
    from langchain.text_splitter import CharacterTextSplitter   
    class MyCustomLoader(BaseLoader):  
        def lazy_load(self):  
            text_splitter = CharacterTextSplitter(separator=""t"", keep_separator=True, chunk_size=12, chunk_overlap=2)  
            docs = [Document(page_content=""woof woof"", metadata={""source"": ""doggy.txt""}), Document(page_content=""woof woof woof"", metadata={""source"": ""doggy.txt""})]  
            yield from text_splitter.split_documents(docs)  

    loader = MyCustomLoader()"
Что такое агенты в LangChain и как они работают?,"Агенты в LangChain используют языковую модель для выбора последовательности действий. В отличие от цепочек, где последовательность действий жестко задана в коде, в агентах языковая модель используется как движок рассуждений для определения какие действия предпринять и в каком порядке&#8203;``【oaicite:3】``&#8203;."
Какие ключевые компоненты используются в агентах LangChain?,"В агентах LangChain используются такие ключевые компоненты, как инструменты (tools), которые агент может вызывать, и наборы инструментов (toolkits), представляющие собой группы из 3-5 инструментов, необходимых для выполнения конкретных задач. Кроме того, используется `AgentExecutor`, который является средой выполнения для агента и управляет выполнением выбранных агентом действий&#8203;``【oaicite:2】``&#8203;."
Как настроить и использовать AgentExecutor в LangChain?,"from langchain.agents import AgentExecutor   
    from langchain.chat_models import ChatOpenAI   
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder   
    from langchain.tools.render import format_tool_to_openai_function   
    from langchain.agents.format_scratchpad import format_to_openai_function_messages   
    from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser   

    llm = ChatOpenAI(model=""gpt-3.5-turbo"", temperature=0)  
    prompt = ChatPromptTemplate.from_messages([...])  
    llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])  
    agent = (...) | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()  

    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
Как добавить память в агента LangChain?,"from langchain.schema.messages import AIMessage, HumanMessage   
    from langchain.prompts import MessagesPlaceholder   

    MEMORY_KEY = ""chat_history""  
    prompt = ChatPromptTemplate.from_messages([...])  
    chat_history = []  

    agent = (...) | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()  
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)  

    input1 = ""how many letters in the word educa?""  
    result = agent_executor.invoke({""input"": input1, ""chat_history"": chat_history})  
    chat_history.extend([...])"
Какие типы агентов доступны в LangChain?,"В LangChain доступны различные типы агентов, использующих языковую модель для определения необходимых действий и их последовательности. Эти агенты включают Zero-shot ReAct, Structured input ReAct, OpenAI Functions, Conversational, Self-ask with search, и ReAct document store. Каждый из этих агентов имеет уникальные функции и применения&#8203;``【oaicite:3】``&#8203;."
Что делает агент Zero-shot ReAct в LangChain?,"Агент Zero-shot ReAct в LangChain использует фреймворк ReAct для определения, какой инструмент использовать, исходя исключительно из описания инструмента. Этот агент требует предоставления описания для каждого инструмента и является наиболее общим агентом для действий&#8203;``【oaicite:2】``&#8203;."
В чем особенность агента Structured input ReAct в LangChain?,"Агент Structured input ReAct в LangChain способен использовать инструменты с множественными входами. В отличие от старых агентов, которые определяют входное действие как одну строку, этот агент может использовать схему аргументов инструментов для создания структурированного входного действия. Это полезно для более сложного использования инструментов, например, для точной навигации в браузере&#8203;``【oaicite:1】``&#8203;."
Как работает агент OpenAI Functions в LangChain?,"Агент OpenAI Functions в LangChain разработан для работы с моделями OpenAI (например, gpt-3.5-turbo-0613 и gpt-4-0613), которые были специально дообучены для обнаружения моментов вызова функций и формирования входных данных для этих функций&#8203;``【oaicite:0】``&#8203;."
Что такое разговорный агент в LangChain и в чем его особенности?,"Разговорный агент в LangChain оптимизирован для разговорных сценариев. В отличие от других агентов, которые оптимизированы для использования инструментов для нахождения лучшего ответа, разговорный агент может вести беседу с пользователем. Основное отличие от стандартного ReAct агента заключается в более разговорном стиле подсказок&#8203;``【oaicite:3】``&#8203;."
Как создать разговорного агента в LangChain с использованием LCEL?,"from langchain import hub   
    from langchain.agents.format_scratchpad import format_log_to_str   
    from langchain.agents.output_parsers import ReActSingleInputOutputParser   
    from langchain.tools.render import render_text_description   
    from langchain.llms import OpenAI   
    from langchain.memory import ConversationBufferMemory   
    from langchain.utilities import SerpAPIWrapper   

    prompt = hub.pull(""hwchase17/react-chat"")  
    prompt = prompt.partial(...)  
    llm_with_stop = OpenAI(temperature=0).bind(stop=[""
Observation""])  
    agent = (...) | prompt | llm_with_stop | ReActSingleInputOutputParser()"
Как использовать стандартный класс агента для создания разговорного агента в LangChain?,"
    from langchain.agents import AgentType, initialize_agent   
    from langchain.memory import ConversationBufferMemory   

    agent_executor = initialize_agent(  
        tools,  
        llm,  
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,  
        verbose=True,  
        memory=ConversationBufferMemory(memory_key=""chat_history""),  
    )  
    "
Как использовать модель чата для создания разговорного агента в LangChain?,"from langchain import hub   
    from langchain.chat_models import ChatOpenAI   
    from langchain.agents.format_scratchpad import format_log_to_messages   
    from langchain.agents.output_parsers import JSONAgentOutputParser   
    from langchain.agents import AgentExecutor   
    from langchain.memory import ConversationBufferMemory   

    prompt = hub.pull(""hwchase17/react-chat-json"")  
    chat_model = ChatOpenAI(temperature=0, model=""gpt-4"")  
    prompt = prompt.partial(...)  
    chat_model_with_stop = chat_model.bind(stop=[""
Observation""])  
    agent = (...) | prompt | chat_model_with_stop | JSONAgentOutputParser()  
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=ConversationBufferMemory(memory_key=""chat_history"", return_messages=True))"
Что такое OpenAI Assistants в LangChain?,"OpenAI Assistants в LangChain позволяют создавать AI-ассистентов в собственных приложениях. Эти ассистенты могут использовать инструкции, модели, инструменты и знания для ответа на пользовательские запросы. Assistants API поддерживает три типа инструментов: интерпретатор кода, извлечение и вызов функций&#8203;``【oaicite:5】``&#8203;."
Как использовать только инструменты OpenAI с OpenAI Assistants в LangChain?,"from langchain.agents.openai_assistant import OpenAIAssistantRunnable   

    interpreter_assistant = OpenAIAssistantRunnable.create_assistant(  
        name=""langchain assistant"",  
        instructions=""You are a personal math tutor. Write and run code to answer math questions."",  
        tools=[{""type"": ""code_interpreter""}],  
        model=""gpt-4-1106-preview"",  
    )  
    output = interpreter_assistant.invoke({""content"": ""What's 10 - 4 raised to the 2.7""})"
Как использовать AgentExecutor с OpenAI Assistants в LangChain?,"from langchain.agents import AgentExecutor   

    agent_executor = AgentExecutor(agent=agent, tools=tools)  
    agent_executor.invoke({""content"": ""What's the weather in SF today divided by 2.7""})"
Как настроить пользовательский цикл выполнения для OpenAI Assistants в LangChain?,С помощью LCEL можно легко написать собственный цикл выполнения для запуска ассистента. Это дает полный контроль над выполнением&#8203;``【oaicite:2】``&#8203;.
Как использовать существующий поток или ассистента в OpenAI Assistants в LangChain?,"# Использование существующего потока
    next_response = execute_agent(  
        agent,  
        tools,  
        {""content"": ""now add 17.241"", ""thread_id"": response.return_values[""thread_id""]},  
    )  

    # Использование существующего ассистента
    agent = OpenAIAssistantRunnable(assistant_id=""<ASSISTANT_ID>"", as_agent=True)"
Что такое OpenAI Functions Agent в LangChain?,"OpenAI Functions Agent в LangChain предназначен для работы с моделями OpenAI, такими как gpt-3.5-turbo-0613 и gpt-4-0613, которые были специально дообучены для определения момента вызова функции и предоставления входных данных для этой функции&#8203;``【oaicite:1】``&#8203;."
Как использовать OpenAI Functions Agent в LangChain?,"from langchain.agents import initialize_agent   
    from langchain.agents import AgentType   

    agent_executor = initialize_agent(  
        tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True  
    )  

    agent_executor.invoke({  
        ""input"": ""Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?""  
    })"
Как работают инструменты OpenAI в LangChain?,Инструменты OpenAI в LangChain позволяют взаимодействовать с OpenAI Assistants с использованием инструментов OpenAI или пользовательских инструментов. При использовании исключительно инструментов OpenAI можно непосредственно вызывать ассистента и получать окончательные ответы. При использовании пользовательских инструментов можно запускать цикл выполнения ассистента и инструментов с помощью встроенного AgentExecutor или легко написать свой собственный исполнитель&#8203;``【oaicite:1】``&#8203;.
Как использовать AgentExecutor с инструментами OpenAI в LangChain?,"from langchain.agents import AgentExecutor   

    agent_executor = AgentExecutor(agent=agent, tools=tools)  
    agent_executor.invoke({""content"": ""What's the weather in SF today divided by 2.7""})"
Что такое ReAct Agent в LangChain и как он используется?,"ReAct Agent в LangChain используется для реализации логики ReAct, используя агента для управления последовательностью действий. Этот подход позволяет загружать инструменты и использовать языковую модель для управления агентом. ReAct Agent позволяет создавать более динамичные и гибкие взаимодействия с пользователем&#8203;``【oaicite:2】``&#8203;."
Как создать ReAct Agent в LangChain с использованием LCEL?,"from langchain import hub   
    from langchain.agents.format_scratchpad import format_log_to_str   
    from langchain.agents.output_parsers import ReActSingleInputOutputParser   
    from langchain.tools.render import render_text_description   
    from langchain.llms import OpenAI   
    from langchain.agents import AgentExecutor   

    llm = OpenAI(temperature=0)  
    prompt = hub.pull(""hwchase17/react"")  
    agent = (...) | prompt | llm_with_stop | ReActSingleInputOutputParser()  
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
Как использовать ZeroShotReactAgent в LangChain?,"from langchain.agents import initialize_agent   
    from langchain.agents import AgentType   

    agent_executor = initialize_agent(  
        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True  
    )  
    agent_executor.invoke({""input"": ""Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?""})"
Что такое Self-ask with Search Agent в LangChain и как он работает?,"Self-ask with Search Agent в LangChain - это цепочка, которая демонстрирует возможность самозадаваемого поиска. Этот агент использует инструменты, такие как поиск с помощью SerpAPI, для ответа на вопросы, задавая последовательные вопросы и используя результаты поиска для формирования ответов&#8203;``【oaicite:1】``&#8203;."
Как создать Self-ask with Search Agent в LangChain с использованием LangChain Expression Language?,"from langchain.agents import AgentType, Tool, initialize_agent   
    from langchain.llms import OpenAI   
    from langchain.utilities import SerpAPIWrapper   
    from langchain import hub   
    from langchain.agents.format_scratchpad import format_log_to_str   
    from langchain.agents.output_parsers import SelfAskOutputParser   
    from langchain.agents import AgentExecutor   

    llm = OpenAI(temperature=0)  
    search = SerpAPIWrapper()  
    tools = [Tool(name=""Intermediate Answer"", func=search.run, description=""useful for when you need to ask with search"")]  
    prompt = hub.pull(""hwchase17/self-ask-with-search"")  
    llm_with_stop = llm.bind(stop=[""
Intermediate answer:""])  
    agent = (...) | prompt | llm_with_stop | SelfAskOutputParser()  
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
Что такое Structured Tool Chat Agent в LangChain и как он работает?,"Structured Tool Chat Agent в LangChain способен использовать инструменты с множественными входными данными. В отличие от более старых агентов, которые определяют входное действие как одну строку, этот агент может использовать схему аргументов инструментов для создания структурированного входного действия. Это полезно для более сложного использования инструментов&#8203;``【oaicite:1】``&#8203;."
Как создать и использовать Structured Tool Chat Agent в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent, AgentExecutor   
    from langchain.chat_models import ChatOpenAI   
    from langchain.utilities import PlayWrightBrowserToolkit   
    from langchain.llms import OpenAI   
    from langchain import hub   
    from langchain.agents.format_scratchpad import format_log_to_str   
    from langchain.agents.output_parsers import JSONAgentOutputParser   

    llm = ChatOpenAI(temperature=0)  
    prompt = hub.pull(""hwchase17/react-multi-input-json"")  
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
Что такое XML Agent в LangChain и как он работает?,"XML Agent в LangChain использует логику XML для управления запросами и ответами с агентом. Это особенно полезно для моделей языка, которые хорошо работают с XML, таких как Claude от Anthropic. Агент позволяет создавать структурированные запросы и ответы, используя XML-форматирование&#8203;``【oaicite:1】``&#8203;."
Как создать и использовать XML Agent в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent, AgentExecutor   
    from langchain.chat_models import ChatAnthropic   
    from langchain import hub   
    from langchain.agents.format_scratchpad import format_xml   
    from langchain.agents.output_parsers import XMLAgentOutputParser   

    model = ChatAnthropic(model=""claude-2"")  
    prompt = hub.pull(""hwchase17/xml-agent"")  
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
Как использовать функциональность 'AgentExecutorIterator' в LangChain?,"Для демонстрации функциональности 'AgentExecutorIterator', в LangChain можно настроить проблему, в которой агенту необходимо получить три простых числа с помощью инструмента и умножить их вместе. В этой простой задаче можно добавить логику для проверки промежуточных шагов, проверяя, являются ли их результаты простыми числами&#8203;``【oaicite:1】``&#8203;."
Как настроить и выполнить итерацию агента в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent   
    from langchain.llms import ChatOpenAI   
    from langchain.chains import LLMMathChain   
    from pydantic.v1 import BaseModel, Field   

    llm = ChatOpenAI(temperature=0, model=""gpt-4"")  
    llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)  
    tools = [...]  
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)  

    question = ""What is the product of the 998th, 999th and 1000th prime numbers?""  
    for step in agent.iter(question):  
        # Custom logic for checking steps"
Какова цель возвращения структурированного вывода с помощью агента в LangChain?,"Целью возвращения структурированного вывода с помощью агента в LangChain является обеспечение возможности агентам предоставлять ответы в структурированном формате, а не только в виде одной строки. Это особенно полезно для задач, требующих подробной и организованной информации, таких как ответы на вопросы с указанием источников информации."
Как структурированный вывод повышает функциональность агента в LangChain?,"Структурированный вывод повышает функциональность агента в LangChain, организуя его ответы более информативно и структурированно. Это особенно важно для задач, которые требуют подробной информации и цитирования источников."
Какова цель объединения агентов и векторных хранилищ в LangChain?,"Цель объединения агентов и векторных хранилищ в LangChain - создать агента, который может взаимодействовать с данными, индексированными в векторном хранилище, и обеспечивать агентное взаимодействие для поиска и извлечения данных&#8203;``【oaicite:2】``&#8203;."
Как создать агента для работы с векторным хранилищем в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent   
    from langchain.llms import OpenAI   
    from langchain.chains import RetrievalQA   
    from langchain.vectorstores import Chroma   
    from langchain.document_loaders import TextLoader   
    from langchain.text_splitter import CharacterTextSplitter   
    from langchain.embeddings.openai import OpenAIEmbeddings   

    llm = OpenAI(temperature=0)  
    docsearch = Chroma.from_documents(...)  
    state_of_union = RetrievalQA.from_chain_type(llm=llm, retriever=docsearch.as_retriever())  
    tools = [Tool(name=""State of Union QA System"", func=state_of_union.run, ...)]  
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
Как использовать агента в LangChain в качестве маршрутизатора для векторных хранилищ?,"
    tools = [Tool(name=""State of Union QA System"", func=state_of_union.run, return_direct=True), ...]  
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)  
    agent.run(""What did biden say about ketanji brown jackson in the state of the union address?"")  
    "
Какова цель использования асинхронного API с агентами в LangChain?,"Асинхронный API в LangChain используется для улучшения производительности агентов, позволяя им выполнять задачи параллельно. Это особенно полезно для задач, требующих одновременного выполнения нескольких запросов или операций&#8203;``【oaicite:3】``&#8203;."
Какие инструменты поддерживают асинхронные методы в LangChain?,"Асинхронные методы в LangChain поддерживаются для инструментов, таких как GoogleSerperAPIWrapper, SerpAPIWrapper, LLMMathChain и Qdrant. Это позволяет агентам асинхронно вызывать эти инструменты, улучшая обработку запросов&#8203;``【oaicite:2】``&#8203;."
Как работает асинхронный AgentExecutor в LangChain?,"
    agent_executor = AgentExecutor(agent=agent, tools=tools)  
    await agent_executor.arun({""input"": ""Some question""})  
    "
Как асинхронный API ускоряет выполнение задач в LangChain?,"Асинхронный API ускоряет выполнение задач в LangChain, позволяя агентам выполнять несколько запросов параллельно, а не последовательно. Это значительно сокращает время выполнения, особенно при обработке нескольких запросов&#8203;``【oaicite:0】``&#8203;."
Как можно создать клон ChatGPT в LangChain?,"Для создания клона ChatGPT в LangChain используется комбинация определенного шаблона запроса и концепции памяти. Пример демонстрирует, как можно использовать LLMChain с моделью OpenAI и шаблоном запроса, который включает память, для создания агента, способного вести разговор и реагировать на запросы пользователя, подобно ChatGPT&#8203;``【oaicite:2】``&#8203;."
Каковы ключевые компоненты для создания клона ChatGPT в LangChain?,"from langchain.chains import LLMChain   
    from langchain.llms import OpenAI   
    from langchain.memory import ConversationBufferWindowMemory   
    from langchain.prompts import PromptTemplate   

    template = """"""Assistant is a large language model trained by OpenAI. ... 
    Human: {human_input}  
    Assistant:""""""  

    prompt = PromptTemplate(input_variables=[""history"", ""human_input""], template=template)  
    chatgpt_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt, verbose=True, memory=ConversationBufferWindowMemory(k=2))"
Как работает клон ChatGPT в LangChain при взаимодействии с пользователем?,"Клон ChatGPT в LangChain работает так, что реагирует на ввод пользователя, предоставляя ответы в стиле терминала Linux или других специфичных запросов. Агент использует заданный шаблон запроса и память для генерации соответствующих и релевантных ответов&#8203;``【oaicite:0】``&#8203;."
Как интегрировать пользовательские функции с OpenAI Functions Agent в LangChain?,"Интеграция пользовательских функций с OpenAI Functions Agent в LangChain включает определение пользовательских функций, таких как получение текущей цены на акции и процентное изменение цены на акции за определенный период времени, и использование этих функций в качестве инструментов в агенте&#8203;``【oaicite:2】``&#8203;."
Как создать агента с пользовательскими инструментами в LangChain?,"from langchain.agents import AgentType, initialize_agent   
    from langchain.chat_models import ChatOpenAI   

    llm = ChatOpenAI(model=""gpt-3.5-turbo-0613"", temperature=0)  
    tools = [CurrentStockPriceTool(), StockPerformanceTool()]  
    agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)"
Какие примеры использования агента с пользовательскими функциями в LangChain?,"# Пример запроса текущей цены на акции Microsoft и их производительности за последние 6 месяцев
    agent.run(""What is the current price of Microsoft stock? How it has performed over past 6 months?"")  
    
    # Пример запроса недавних цен на акции Google и Meta
    agent.run(""Give me recent stock prices of Google and Meta?"")  
    
    # Сравнение производительности акций Microsoft и Google за последние 3 месяца
    agent.run(""In the past 3 months, which stock between Microsoft and Google has performed the best?"")"
Как создать собственного настраиваемого агента в LangChain?,"Для создания настраиваемого агента в LangChain необходимо определить две основные части: инструменты, которые будут доступны агенту для использования, и сам класс агента, который определяет, какое действие предпринять. Пример включает создание агента с использованием класса `BaseSingleActionAgent` и инструмента `Tool`, который выполняет функцию поиска&#8203;``【oaicite:2】``&#8203;."
Какие компоненты используются для создания настраиваемого агента в LangChain?,"
    from langchain.agents import AgentExecutor, BaseSingleActionAgent, Tool   
    from langchain.utilities import SerpAPIWrapper   

    search = SerpAPIWrapper()  
    tools = [  
        Tool(name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events"", return_direct=True)  
    ]  

    class FakeAgent(BaseSingleActionAgent):  
        ...  
        def plan(self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) -> Union[AgentAction, AgentFinish]:  
            ...  
            return AgentAction(tool=""Search"", tool_input=kwargs[""input""], log="""")  

    agent = FakeAgent()  
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)  
    "
Как выполняется задача с использованием настраиваемого агента в LangChain?,"agent_executor.run(""How many people live in canada as of 2023?"")  

    # Вывод: 'The current population of Canada is 38,669,152 as of Monday, April 24, 2023, based on Worldometer elaboration of the latest United Nations data.'"
Какова цель создания пользовательского агента с извлечением инструментов в LangChain?,"Цель создания пользовательского агента с извлечением инструментов в LangChain - использовать векторное хранилище для создания векторных представлений описаний каждого инструмента. Затем, для входящего запроса создаются векторные представления, которые используются для поиска по схожести с целью нахождения релевантных инструментов&#8203;``【oaicite:2】``&#8203;."
Как настроить базовый шаблон запроса для пользовательского агента в LangChain?,"
    from typing import Callable   
    from langchain.prompts import StringPromptTemplate   

    class CustomPromptTemplate(StringPromptTemplate):  
        template: str  
        tools_getter: Callable  

        def format(self, **kwargs) -> str:  
            intermediate_steps = kwargs.pop(""intermediate_steps"")  
            thoughts = """"  
            for action, observation in intermediate_steps:  
                thoughts += action.log  
                thoughts += f""
Observation: {observation}
Thought: ""  
            kwargs[""agent_scratchpad""] = thoughts  
            tools = self.tools_getter(kwargs[""input""])  
            kwargs[""tools""] = ""
"".join([f""{tool.name}: {tool.description}"" for tool in tools])  
            kwargs[""tool_names""] = "", "".join([tool.name for tool in tools])  
            return self.template.format(**kwargs)  

    prompt = CustomPromptTemplate(template=template, tools_getter=get_tools, input_variables=[""input"", ""intermediate_steps""])  
    "
Как работает пользовательский агент с извлечением инструментов в LangChain при выполнении запроса?,"from langchain.agents import AgentExecutor, LLMSingleActionAgent   
    from langchain.llms import OpenAI   
    from langchain.chains import LLMChain   
    from langchain.prompts import CustomPromptTemplate   
    from langchain.utilities import SerpAPIWrapper   

    llm = OpenAI(temperature=0)  
    llm_chain = LLMChain(llm=llm, prompt=prompt)  
    tools = get_tools(""whats the weather?"")  
    agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser, stop=[""
Observation:""], allowed_tools=[tool.name for tool in tools])  
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)  
    agent_executor.run(""What's the weather in SF?"")"
Какова цель создания пользовательского LLM агента в LangChain?,"Цель создания пользовательского LLM агента в LangChain - разработать агента, который использует языковую модель для определения действий, необходимых для ответа на вопросы пользователя. Это включает в себя настройку шаблона запроса, выбора языковой модели, определения последовательности остановки и парсинга вывода модели&#8203;``【oaicite:2】``&#8203;."
Как настроить шаблон запроса для пользовательского LLM агента в LangChain?,"
    from langchain.prompts import StringPromptTemplate   
    class CustomPromptTemplate(StringPromptTemplate):  
        template: str  
        tools: List[Tool]  
        def format(self, **kwargs) -> str:  
            intermediate_steps = kwargs.pop(""intermediate_steps"")  
            thoughts = """"  
            for action, observation in intermediate_steps:  
                thoughts += action.log  
                thoughts += f""
Observation: {observation}
Thought: ""  
            kwargs[""agent_scratchpad""] = thoughts  
            kwargs[""tools""] = ""
"".join([f""{tool.name}: {tool.description}"" for tool in self.tools])  
            kwargs[""tool_names""] = "", "".join([tool.name for tool in self.tools])  
            return self.template.format(**kwargs)  
    prompt = CustomPromptTemplate(template=template, tools=tools, input_variables=[""input"", ""intermediate_steps""])  
    "
Как работает пользовательский LLM агент в LangChain при выполнении запроса?,"from langchain.agents import AgentExecutor, LLMSingleActionAgent   
    from langchain.llms import OpenAI   
    from langchain.chains import LLMChain   
    from langchain.prompts import CustomPromptTemplate   
    llm = OpenAI(temperature=0)  
    llm_chain = LLMChain(llm=llm, prompt=prompt)  
    agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser, stop=[""
Observation:""], allowed_tools=[tool.name for tool in tools])  
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)  
    agent_executor.run(""How many people live in canada as of 2023?"")"
Какова цель создания пользовательского LLM чат-агента в LangChain?,"Цель создания пользовательского LLM чат-агента в LangChain - разработать агента, который использует языковую модель для ведения разговора и выполнения задач, основываясь на пользовательских запросах и взаимодействиях. Это включает в себя настройку шаблона запроса, выбора языковой модели и парсинга вывода модели для формирования ответов&#8203;``【oaicite:2】``&#8203;."
Как настроить шаблон запроса для пользовательского LLM чат-агента в LangChain?,"
    from langchain.prompts import BaseChatPromptTemplate   
    class CustomPromptTemplate(BaseChatPromptTemplate):  
        template: str  
        tools: List[Tool]  
        def format_messages(self, **kwargs) -> str:  
            intermediate_steps = kwargs.pop(""intermediate_steps"")  
            thoughts = """"  
            for action, observation in intermediate_steps:  
                thoughts += action.log  
                thoughts += f""
Observation: {observation}
Thought: ""  
            kwargs[""agent_scratchpad""] = thoughts  
            kwargs[""tools""] = ""
"".join([f""{tool.name}: {tool.description}"" for tool in self.tools])  
            kwargs[""tool_names""] = "", "".join([tool.name for tool in self.tools])  
            formatted = self.template.format(**kwargs)  
            return [HumanMessage(content=formatted)]  
    prompt = CustomPromptTemplate(template=template, tools=tools, input_variables=[""input"", ""intermediate_steps""])  
    "
Как работает пользовательский LLM чат-агент в LangChain при выполнении запроса?,"from langchain.agents import AgentExecutor, LLMSingleActionAgent   
    from langchain.llms import ChatOpenAI   
    from langchain.chains import LLMChain   
    from langchain.prompts import CustomPromptTemplate   
    from langchain.utilities import SerpAPIWrapper   

    llm = ChatOpenAI(temperature=0)  
    llm_chain = LLMChain(llm=llm, prompt=prompt)  
    agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser, stop=[""
Observation:""], allowed_tools=[tool.name for tool in tools])  
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)  
    agent_executor.run(""Search for Leo DiCaprio's girlfriend on the internet."")"
Что такое пользовательский MRKL агент в LangChain и из чего он состоит?,"Пользовательский MRKL агент в LangChain состоит из трех частей: инструментов, которые агент может использовать (`Tools`), `LLMChain`, который производит текст, и самого класса агента, который анализирует выходные данные `LLMChain` для определения действий. Это позволяет создать гибкого агента, который может использовать различные инструменты для выполнения задач&#8203;``【oaicite:2】``&#8203;."
Как создать пользовательский `LLMChain` для MRKL агента в LangChain?,"Для создания пользовательского `LLMChain` для MRKL агента в LangChain рекомендуется использовать существующий класс агента, но с настраиваемым `LLMChain`. Большая часть работы заключается в создании шаблона запроса, который должен производить текст в определенном формате, и использовании `agent_scratchpad` для заметок о предыдущих действиях и наблюдениях&#8203;``【oaicite:1】``&#8203;."
Как работает настроенный MRKL агент в LangChain?,"from langchain.agents import AgentExecutor, Tool, ZeroShotAgent   
    from langchain.chains import LLMChain   
    from langchain.llms import OpenAI   
    from langchain.utilities import SerpAPIWrapper   

    search = SerpAPIWrapper()  
    tools = [...]  

    prompt = ZeroShotAgent.create_prompt(tools, ...)  

    llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)  
    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=[tool.name for tool in tools])  
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)  
    agent_executor.run(""Вопрос"")  

    # Вывод: результат выполнения запроса с использованием заданных инструментов и логики MRKL агента"
Какова структура пользовательского многошагового агента в LangChain?,"Пользовательский многошаговый агент в LangChain состоит из двух основных частей: инструментов (`Tools`), которые агент может использовать, и самого класса агента, который определяет, какое действие предпринять. Этот подход позволяет создать гибкого агента, способного предсказывать или предпринимать несколько шагов за раз&#8203;``【oaicite:2】``&#8203;."
Как создать пользовательского многошагового агента в LangChain?,"from langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool   
    from langchain.utilities import SerpAPIWrapper   

    def random_word(query: str) -> str:  
        return ""foo""  

    search = SerpAPIWrapper()  
    tools = [  
        Tool(name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events""),  
        Tool(name=""RandomWord"", func=random_word, description=""call this to get a random word."")  
    ]  

    class FakeAgent(BaseMultiActionAgent):  
        def plan(self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) -> Union[List[AgentAction], AgentFinish]:  
            if len(intermediate_steps) == 0:  
                return [AgentAction(tool=""Search"", tool_input=kwargs[""input""], log=""""), AgentAction(tool=""RandomWord"", tool_input=kwargs[""input""], log="""")]  
            else:  
                return AgentFinish(return_values={""output"": ""bar""}, log="""")  

    agent = FakeAgent()  
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
Как работает пользовательский многошаговый агент при выполнении запроса в LangChain?,"Пользовательский многошаговый агент в LangChain выполняет запрос, используя определенные инструменты и логику планирования. Например, он может сначала использовать инструмент для поиска, а затем другой инструмент для получения случайного слова, прежде чем предоставить окончательный ответ&#8203;``【oaicite:0】``&#8203;."
Какие проблемы могут возникнуть при разборе ответов LLM в LangChain?,"При использовании LLM с агентами в LangChain могут возникнуть ошибки из-за неправильно отформатированных выводов LLM, которые не соответствуют ожидаемому формату парсера вывода. Это может привести к сбою агента&#8203;``【oaicite:2】``&#8203;."
Как можно обрабатывать ошибки парсинга в LangChain?,"В LangChain для обработки ошибок парсинга можно использовать параметр `handle_parsing_errors`. По умолчанию, если возникает ошибка парсинга, агент завершается с ошибкой, но этот параметр позволяет контролировать эту функциональность и обрабатывать ошибки более гибко&#8203;``【oaicite:1】``&#8203;."
Как настроить собственную обработку ошибок парсинга в LangChain?,"# Пример настройки собственного сообщения об ошибке
    mrkl = initialize_agent(
        tools, 
        ChatOpenAI(temperature=0), 
        agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, 
        verbose=True, 
        handle_parsing_errors=""Check your output and make sure it conforms!"",
    )  

    # Пример использования функции для обработки ошибок
    def _handle_error(error) -> str:
        return str(error)[:50]

    mrkl = initialize_agent(
        tools, 
        ChatOpenAI(temperature=0), 
        agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, 
        verbose=True, 
        handle_parsing_errors=_handle_error,
    )"
Как можно получить доступ к промежуточным шагам агента в LangChain?,"В LangChain можно получить доступ к промежуточным шагам агента, используя параметр `return_intermediate_steps=True` при инициализации агента. Это позволяет отслеживать каждый шаг в цепочке действий агента, включая использование инструментов и наблюдения за результатами&#8203;``【oaicite:2】``&#8203;."
Каков пример использования агента с доступом к промежуточным шагам в LangChain?,"from langchain.agents import AgentType, initialize_agent, load_tools   
    from langchain.llms import OpenAI   

    llm = OpenAI(temperature=0, model_name=""text-davinci-002"")  
    tools = load_tools([""serpapi"", ""llm-math""], llm=llm)  

    agent = initialize_agent(  
        tools,  
        llm,  
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  
        verbose=True,  
        return_intermediate_steps=True,  
    )  

    response = agent({""input"": ""Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?""})  

    print(response[""intermediate_steps""])"
Каковы преимущества использования промежуточных шагов в LangChain?,"Использование промежуточных шагов в LangChain позволяет лучше понять, как агент приходит к окончательному ответу. Это обеспечивает дополнительную прозрачность и позволяет пользователям или разработчикам увидеть, какие инструменты использовались и какие наблюдения были сделаны на каждом шаге процесса&#8203;``【oaicite:0】``&#8203;."
Какова цель ограничения максимального количества итераций в агентах LangChain?,"Цель ограничения максимального количества итераций в агентах LangChain - предотвратить бесконечные или излишне длительные циклы выполнения, особенно в случаях, когда агент может 'зациклиться' на определенных задачах или вводах. Это улучшает управление процессом и повышает эффективность работы агентов&#8203;``【oaicite:2】``&#8203;."
Как настроить максимальное количество итераций для агента в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent   
    from langchain.llms import OpenAI   

    llm = OpenAI(temperature=0)  
    tools = [...]  
    agent = initialize_agent(  
        tools,  
        llm,  
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  
        verbose=True,  
        max_iterations=2,  
    )"
Каковы способы раннего завершения итераций в LangChain?,"В LangChain есть два способа раннего завершения итераций: метод 'force', который просто возвращает постоянную строку, и метод 'generate', который выполняет один ФИНАЛЬНЫЙ проход через LLM для генерации вывода. Это позволяет управлять процессом выполнения и предоставлять пользователю информативный ответ при достижении лимита итераций&#8203;``【oaicite:0】``&#8203;."
Какова цель установления максимального времени выполнения для агентов в LangChain?,"Цель установления максимального времени выполнения для агентов в LangChain - предотвратить чрезмерно длительные или бесконечные процессы выполнения агентом, особенно в случаях, когда агент может войти в бесконечный цикл или занять слишком много времени для решения задачи. Это помогает обеспечить эффективное и контролируемое выполнение задач агентом&#8203;``【oaicite:2】``&#8203;."
Как настроить максимальное время выполнения для агента в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent   
    from langchain.llms import OpenAI   

    llm = OpenAI(temperature=0)  
    tools = [Tool(name=""Jester"", func=lambda x: ""foo"", description=""useful for answer the question"")]  
    agent = initialize_agent(  
        tools,  
        llm,  
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  
        verbose=True,  
        max_execution_time=1,  # Установка лимита времени выполнения в 1 секунду
    )"
Как агент LangChain ведет себя при достижении лимита времени выполнения?,"При достижении установленного лимита времени выполнения, агент LangChain завершает выполнение, используя метод 'force' (по умолчанию), который возвращает постоянную строку. В качестве альтернативы, можно указать метод 'generate', который выполнит один последний проход через LLM для генерации вывода&#8203;``【oaicite:0】``&#8203;."
Что демонстрирует документация по репликации MRKL с использованием агентов в LangChain?,"Документация по репликации MRKL в LangChain демонстрирует, как использовать агентов для решения задач, подобных системе MRKL. Это включает использование инструментов для поиска, вычисления и взаимодействия с базами данных для получения ответов на сложные вопросы&#8203;``【oaicite:2】``&#8203;."
Как настроить агента для репликации MRKL в LangChain?,"from langchain.chains import LLMMathChain   
    from langchain.llms import OpenAI   
    from langchain.utilities import SerpAPIWrapper, SQLDatabase   
    from langchain_experimental.sql import SQLDatabaseChain   
    from langchain.agents import initialize_agent, Tool, AgentType   

    llm = OpenAI(temperature=0)  
    search = SerpAPIWrapper()  
    llm_math_chain = LLMMathChain(llm=llm, verbose=True)  
    db = SQLDatabase.from_uri(""sqlite:///../../../../../notebooks/Chinook.db"")  
    db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)  
    tools = [Tool(name=""Search"", func=search.run, description=""...""), Tool(name=""Calculator"", func=llm_math_chain.run, description=""...""), Tool(name=""FooBar DB"", func=db_chain.run, description=""..."")]  

    mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
Как работает агент MRKL в LangChain при обработке запросов?,"# Пример выполнения запроса 'Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?'
    mrkl.run(""Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?"")  
    # Вывод: 'Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.7030049853137306.'

    # Пример выполнения запроса 'What is the full name of the artist who recently released an album called 'The Storm Before the Calm' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database?'
    mrkl.run(""What is the full name of the artist who recently released an album called 'The Storm Before the Calm' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database?"")  
    # Вывод: 'The artist who released the album 'The Storm Before the Calm' is Alanis Morissette and the albums of hers in the FooBar database are Jagged Little Pill.'"
Какие модули необходимо импортировать для работы с общей памятью между агентами и инструментами в Langchain?,"
from langchain.agents import AgentExecutor, Tool, ZeroShotAgent   
from langchain.chains import LLMChain   
from langchain.llms import OpenAI   
from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory   
from langchain.prompts import PromptTemplate   
from langchain.utilities import GoogleSearchAPIWrapper   
"
"Как создать шаблон запроса, который включает в себя историю чата и запрашивает создание резюме беседы?","
template = ""This is a conversation between a human and a bot:  

{chat_history}  

Write a summary of the conversation for {input}:  
""  

prompt = PromptTemplate(input_variables=[""input"", ""chat_history""], template=template)  
memory = ConversationBufferMemory(memory_key=""chat_history"")  
readonlymemory = ReadOnlySharedMemory(memory=memory)  
summary_chain = LLMChain(  
    llm=OpenAI(),  
    prompt=prompt,  
    verbose=True,  
    memory=readonlymemory,  # use the read-only memory to prevent the tool from modifying the memory  
)
"
Как настроить инструменты поиска и резюмирования с использованием GoogleSearchAPIWrapper и LLMChain?,"
search = GoogleSearchAPIWrapper()  
tools = [  
    Tool(  
        name=""Search"",  
        func=search.run,  
        description=""useful for when you need to answer questions about current events"",  
    ),  
    Tool(  
        name=""Summary"",  
        func=summary_chain.run,  
        description=""useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary."",  
    ),  
]
"
Как создать пользовательский запрос для ZeroShotAgent в Langchain?,"
prefix = ""Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:""  
suffix = ""Begin!""  

{chat_history}  
Question: {input}  
{agent_scratchpad}""  

prompt = ZeroShotAgent.create_prompt(  
    tools,  
    prefix=prefix,  
    suffix=suffix,  
    input_variables=[""input"", ""chat_history"", ""agent_scratchpad""],
"
Как запустить агентскую цепочку в Langchain для вопроса 'Что такое ChatGPT?'?,"llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)  
agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)  
agent_chain = AgentExecutor.from_agent_and_tools(  
    agent=agent, tools=tools, verbose=True, memory=memory  
)  

agent_chain.run(input=""What is ChatGPT?"")"
"Как задать последующий вопрос агенту в Langchain, зависящий от информации из предыдущего обмена?","
agent_chain.run(input=""Who developed it?"")
"
Какой коллбэк используется для потоковой передачи только окончательного вывода агента?,FinalStreamingStdOutCallbackHandler
Как настроить LLM для потоковой передачи окончательного вывода с использованием FinalStreamingStdOutCallbackHandler?,"from langchain.agents import AgentType, initialize_agent, load_tools   
    from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler  
    from langchain.llms import OpenAI   

    llm = OpenAI(  
        streaming=True, callbacks=[FinalStreamingStdOutCallbackHandler()], temperature=0  
    )  

    tools = load_tools([""wikipedia"", ""llm-math""], llm=llm)  
    agent = initialize_agent(  
        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False  
    )  
    agent.run(  
        ""It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.""  
    )"
Как настроить пользовательские префиксы ответов в LangChain?,"llm = OpenAI(  
        streaming=True,  
        callbacks=[  
            FinalStreamingStdOutCallbackHandler(answer_prefix_tokens=[""The"", ""answer"", "":""])  
        ],  
        temperature=0,  
    )"
Как определить токенизированную версию пользовательского префикса ответа?,"from langchain.callbacks.base import BaseCallbackHandler   

    class MyCallbackHandler(BaseCallbackHandler):  
        def on_llm_new_token(self, token, **kwargs) -> None:  
            print(f""#{token}#"")  

    llm = OpenAI(streaming=True, callbacks=[MyCallbackHandler()])  
    tools = load_tools([""wikipedia"", ""llm-math""], llm=llm)  
    agent = initialize_agent(  
        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False  
    )  
    agent.run(  
        ""It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.""  
    )"
Как включить потоковую передачу префиксов ответов в LangChain?,"Установите параметр `stream_prefix = True`, чтобы префикс ответа также передавался потоковым способом."
Какие компоненты необходимы для использования OpenAI функционального агента с произвольными наборами инструментов в LangChain?,"
    from langchain.agents import AgentType, initialize_agent   
    from langchain.agents.agent_toolkits import SQLDatabaseToolkit   
    from langchain.chat_models import ChatOpenAI   
    from langchain.schema import SystemMessage   
    from langchain.utilities import SQLDatabase
    "
Как загрузить инструменты и настроить системное сообщение для использования с OpenAI функциональным агентом?,"db = SQLDatabase.from_uri(""sqlite:///../../../../../notebooks/Chinook.db"")  
    toolkit = SQLDatabaseToolkit(llm=ChatOpenAI(), db=db)  
    agent_kwargs = {  
        ""system_message"": SystemMessage(content=""You are an expert SQL data analyst."")  
    }  
    llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613"")  
    agent = initialize_agent(  
        toolkit.get_tools(),  
        llm,  
        agent=AgentType.OPENAI_FUNCTIONS,  
        verbose=True,  
        agent_kwargs=agent_kwargs,  
    )  
    agent.run(""how many different artists are there?"")"
Что такое инструменты в контексте LangChain?,"Инструменты - это функции, которые агенты могут использовать для взаимодействия с миром."
Как загрузить инструменты в LangChain?,"
    from langchain.agents import load_tools   
    tool_names = [...]  
    tools = load_tools(tool_names)
    "
Какие импорты необходимы для создания пользовательских инструментов в LangChain?,"
    from langchain.agents import AgentType, initialize_agent   
    from langchain.chains import LLMMathChain   
    from langchain.chat_models import ChatOpenAI   
    from langchain.tools import BaseTool, StructuredTool, Tool, tool   
    from langchain.utilities import SerpAPIWrapper
    "
"Как определить простой инструмент в LangChain, принимающий одну строку и возвращающий строку?","
    search = SerpAPIWrapper()  
    llm_math_chain = LLMMathChain(llm=llm, verbose=True)  
    tools = [  
        Tool.from_function(  
            func=search.run,  
            name=""Search"",  
            description=""useful for when you need to answer questions about current events""
        ),  
    ]
    "
"Как создать пользовательский инструмент, наследуя класс BaseTool в LangChain?","
    from typing import Optional, Type   
    from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun   

    class CustomSearchTool(BaseTool):  
        name = ""custom_search""  
        description = ""useful for when you need to answer questions about current events""  

        def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:  
            return search.run(query)  

        async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:  
            raise NotImplementedError(""custom_search does not support async"")
    "
Как использовать декоратор @tool для создания инструментов в LangChain?,"
    from langchain.tools import tool  

    @tool   
    def search_api(query: str) -> str:  
        return f""Results for query {query}""  

    @tool(""search"", return_direct=True)  
    def search_api(query: str) -> str:  
        return ""Results""
    "
Как добавить человеческое подтверждение в любой инструмент LangChain?,"
    from langchain.callbacks import HumanApprovalCallbackHandler   
    from langchain.tools import ShellTool   

    tool = ShellTool()  

    print(tool.run(""echo Hello World!""))  

        Hello World!
    "
Как добавить стандартный `HumanApprovalCallbackHandler` к инструменту в LangChain?,"tool = ShellTool(callbacks=[HumanApprovalCallbackHandler()])  

    print(tool.run(""ls /usr""))  

        Do you approve of the following input? Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.  

        ls /usr  
        yes  
        X11  
        X11R6  
        bin  
        lib  
        libexec  
        local  
        sbin  
        share  
        standalone"
Как настроить человеческое подтверждение для определенных инструментов и входных данных в LangChain?,"from langchain.agents import AgentType, initialize_agent, load_tools   
    from langchain.llms import OpenAI   

    def _should_check(serialized_obj: dict) -> bool:  
        return serialized_obj.get(""name"") == ""terminal""  

    def _approve(_input: str) -> bool:  
        if _input == ""echo 'Hello World'"":  
            return True  
        msg = ""Do you approve of the following input? Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.""  
        msg += ""

"" + _input + ""
""  
        resp = input(msg)  
        return resp.lower() in (""yes"", ""y"")  

    callbacks = [HumanApprovalCallbackHandler(should_check=_should_check, approve=_approve)]  

    llm = OpenAI(temperature=0)  
    tools = load_tools([""wikipedia"", ""llm-math"", ""terminal""], llm=llm)  
    agent = initialize_agent(  
        tools,  
        llm,  
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  
    )  

    agent.run(""It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany."", callbacks=callbacks)  

        'Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago.'  

    agent.run(""print 'Hello World' in the terminal"", callbacks=callbacks)  

        'Hello World'"
"Как использовать инструмент, требующий нескольких входных данных, с агентом в LangChain?","import os   

    os.environ[""LANGCHAIN_TRACING""] = ""true""  

    from langchain.agents import AgentType, initialize_agent   
    from langchain.llms import OpenAI   
    from langchain.tools import StructuredTool   

    def multiplier(a: float, b: float) -> float:  
        return a * b   

    tool = StructuredTool.from_function(multiplier)  

    agent_executor = initialize_agent(  
        [tool],  
        llm,  
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,  
        verbose=True,  
    )  

    agent_executor.run(""What is 3 times 4"")"
Как использовать инструменты с форматом строки для обработки нескольких входных данных в LangChain?,"from langchain.agents import AgentType, Tool, initialize_agent   
    from langchain.llms import OpenAI   

    def multiplier(a, b):  
        return a * b   

    def parsing_multiplier(string):  
        a, b = string.split("","")  
        return multiplier(int(a), int(b))  

    llm = OpenAI(temperature=0)  
    tools = [  
        Tool(  
            name=""Multiplier"",  
            func=parsing_multiplier,  
            description=""useful for when you need to multiply two numbers together. The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. For example, `1,2` would be the input if you wanted to multiply 1 by 2."",  
        )  
    ]  
    mrkl = initialize_agent(  
        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True  
    )  

    mrkl.run(""What is 3 times 4"")"
Как инструменты LangChain определяют схему аргументов по умолчанию и как можно указать пользовательскую схему ввода?,"from typing import Any, Dict   

    from langchain.agents import AgentType, initialize_agent   
    from langchain.llms import OpenAI   
    from langchain.tools.requests.tool import RequestsGetTool, TextRequestsWrapper   
    from pydantic import BaseModel, Field, root_validator   

    llm = OpenAI(temperature=0)  

    import tldextract   

    _APPROVED_DOMAINS = {  
        ""langchain"",  
        ""wikipedia"",  
    }  

    class ToolInputSchema(BaseModel):  
        url: str = Field(...)  

        @root_validator  
        def validate_query(cls, values: Dict[str, Any]) -> Dict:  
            url = values[""url""]  
            domain = tldextract.extract(url).domain  
            if domain not in _APPROVED_DOMAINS:  
                raise ValueError(  
                    f""Domain {domain} is not on the approved list:""  
                    f"" {sorted(_APPROVED_DOMAINS)}""  
                )  
            return values   

    tool = RequestsGetTool(  
        args_schema=ToolInputSchema, requests_wrapper=TextRequestsWrapper()  
    )  

    agent = initialize_agent(  
        [tool], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False  
    )  

    answer = agent.run(""What's the main title on langchain.com?"")  
    print(answer)  

        The main title of langchain.com is ""LANG CHAIN ️ Official Home Page"""
Как использовать инструменты LangChain в качестве функций OpenAI?,"from langchain.chat_models import ChatOpenAI   
    from langchain.schema import HumanMessage   

    model = ChatOpenAI(model=""gpt-3.5-turbo-0613"")  

    from langchain.tools import MoveFileTool, format_tool_to_openai_function   

    tools = [MoveFileTool()]  
    functions = [format_tool_to_openai_function(t) for t in tools]  

    message = model.predict_messages(  
        [HumanMessage(content=""move file foo to bar"")], functions=functions  
    )  

    message.additional_kwargs[""function_call""]"
Как LangChain обеспечивает асинхронную поддержку и как это используется с объектами `Runnable`?,"import asyncio   
    import time   

    from langchain.chains import LLMChain   
    from langchain.llms import OpenAI   
    from langchain.prompts import PromptTemplate   

    def generate_serially():  
        llm = OpenAI(temperature=0.9)  
        prompt = PromptTemplate(  
            input_variables=[""product""],  
            template=""What is a good name for a company that makes {product}?"",  
        )  
        chain = LLMChain(llm=llm, prompt=prompt)  
        for _ in range(5):  
            resp = chain.run(product=""toothpaste"")  
            print(resp)  

    async def async_generate(chain):  
        resp = await chain.arun(product=""toothpaste"")  
        print(resp)  

    async def generate_concurrently():  
        llm = OpenAI(temperature=0.9)  
        prompt = PromptTemplate(  
            input_variables=[""product""],  
            template=""What is a good name for a company that makes {product}?"",  
        )  
        chain = LLMChain(llm=llm, prompt=prompt)  
        tasks = [async_generate(chain) for _ in range(5)]  
        await asyncio.gather(*tasks)  

    s = time.perf_counter()  
    await generate_concurrently()  
    elapsed = time.perf_counter() - s   
    print(f""Concurrent executed in {elapsed:0.2f} seconds."")  

    s = time.perf_counter()  
    generate_serially()  
    elapsed = time.perf_counter() - s   
    print(f""Serial executed in {elapsed:0.2f} seconds."")"
"Какие методы вызова предоставляются для классов, унаследованных от `Chain` в LangChain?","chat = ChatOpenAI(temperature=0)  
    prompt_template = ""Tell me a {adjective} joke""  
    llm_chain = LLMChain(llm=chat, prompt=PromptTemplate.from_template(prompt_template))  

    llm_chain(inputs={""adjective"": ""corny""})  

    llm_chain(""corny"", return_only_outputs=True)  

    llm_chain.output_keys  

    llm_chain.run({""adjective"": ""corny""})  

    llm_chain.run(""corny"")  

    llm_chain(""corny"")  
    llm_chain({""adjective"": ""corny""})"